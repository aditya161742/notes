
Q) spring boot internal working ??

Ans) https://codezup.com/key-components-internal-working-of-spring-boot-framework-java/

-> In spring boot Group denotes the package name; Artifact denotes the Application name. The default Group name is com. example, and the default Artifact name is demo.

Q)  Difference between map() and flatmap() ??

Ans)

Stream interface has a map() and flatmap() methods and both have intermediate stream operation and return another stream as method output. Both of the functions map() and flatMap are used for transformation and mapping operations. map() function produces one output for one input value, whereas flatMap() function produces an arbitrary no of values as output (ie zero or more than zero) for each input value.

map() can be used where we have to map the elements of a particular collection to a certain function, and then we need to return the stream which contains the updated results.

Example: Multiplying All the elements of the list by 3 and returning the updated list.

flatMap() can be used where we have to flatten or transform out the string, as we cannot flatten our string using map().

Example: Getting the 1st Character of all the String present in a List of Strings and returning the result in form of a stream.


map() Example:

// making the array list object
        ArrayList<String> fruit = new ArrayList<>();
        fruit.add("Apple");
        fruit.add("mango");
        fruit.add("pineapple");
        fruit.add("kiwi");
        System.out.println("List of fruit-" + fruit);
        
        // lets use map() to convert list of fruit
        List list = fruit.stream()
                        .map(s -> s.length())
                        .collect(Collectors.toList());
        System.out.println("List generated by map-" + list);

Output:

List of fruit-[Apple, mango, pineapple, kiwi]
List generated by map-[5, 5, 9, 4]


flatMap() Example:

// making the arraylist object of List of Integer
        List<List<Integer> > number = new ArrayList<>();
        
        // adding the elements to number arraylist
        number.add(Arrays.asList(1, 3));
        number.add(Arrays.asList(3, 8));
        number.add(Arrays.asList(5, 4));
        number.add(Arrays.asList(7, 8));
        
        System.out.println("List of list-" + number);
        
        // using flatmap() to flatten this list
        List<Integer> flatList
            = number.stream()
                  .flatMap(list -> list.stream())
                  .collect(Collectors.toList());
        
        // printing the list
        System.out.println("List generate by flatMap-"
                           + flatList);

Output:

List of list-[[1, 3], [3, 8], [5, 4], [7, 8]]
List generate by flatMap-[1, 3, 3, 8, 5, 4, 7, 8]

Use (for sorting):

number.stream()
                  .flatMap(list -> list.stream())
                  .sorted()
                  .collect(Collectors.toList());

Use (for reverse sorting):

number.sorted( Comparator.reverseOrder() )
                 .forEach(System.out::println);

flatMap() Example 2 :


// Creating a List of Strings
        List<String> list = Arrays.asList("Geeks", "GFG",
                                 "GeeksforGeeks", "gfg");
  
        // Using Stream flatMap(Function mapper)
        list.stream().flatMap(str -> 
                         Stream.of(str.charAt(2))).
                         forEach(System.out::println);

Example:

e
G
e
g





Q)  Difference between @Bean and @Component??

Ans)

-> @Component auto detects and configures the beans using classpath scanning whereas @Bean explicitly declares a single bean, rather than letting Spring do it automatically.

-> @Component need not to be used with the @Configuration annotation where as @Bean annotation has to be used within the class which is annotated with @Configuration

-> @Component has different specializations like @Controller, @Repository and @Service whereas @Bean has no specializations.


Q)  Benefits of spring mvc over other mvc frameworks??

Ans)

Spring boot has dev tools and it contains internal containers like tomcat Getty and using spring boot one can develop production grade application fastly and shipping to prod is also fine and it has inbuilt health checking tools which aids to monitor

-> Spring provides an integrated framework for all tiers of your application, like spring cloud, spring security etc.

-> Spring Controllers are configured using IoC like any other objects. This makes them easy to test and integrated with other objects managed by Spring.



Q) What is viewresolver pattern and how does it works?

Ans)

View Resolver pattern is a J2EE pattern that enables a web application to dynamically select its view technology. For example, HTML, JSP, Tapestry, JSF, XSLT etc. In this pattern, View resolver holds mapping of different views, controller return name of the view, which is then delegated to the View Resolver for selecting an appropriate view. Spring MVC framework supplies inbuilt view resolver for selecting views.

View resolver can also be configured in application.properties file of Spring-Boot web applications, something like below:

spring.mvc.view.prefix=/WEB-INF/jsp/
spring.mvc.view.suffix=.jsp



Q)  what is Scope reference to spring bean?

Ans)

Bean Scopes refers to the lifecycle of Bean that means when the object of Bean will be instantiated, how long does that object live, and how many objects will be created for that bean throughout. Basically, it controls the instance creation of the bean and it is managed by the spring container.

The spring framework provides five scopes for a bean.

1) Singleton: Only one instance will be created for a single bean definition per Spring IoC container and the same object will be shared for each request made for that bean.

Exanple:

public class Person {
    private String name;

    // standard constructor, getters and setters
}

@Bean
@Scope("singleton")
public Person personSingleton() {
    return new Person();
}

2) Prototype: A new instance will be created for a single bean definition every time a request is made for that bean.

Example:

@Bean
@Scope("prototype")
public Person personPrototype() {
    return new Person();
}


3) Request: In request scope, container creates a new instance for each and every HTTP request. So, if server is currently handling 50 requests, then container can have at most 50 individual instances of bean class. Any state change to one instance, will not be visible to other instances. These instances are destructed as soon as the request is completed.

@Component
@Scope("request")
public class BeanClass {
}
 

4) Session: In session scope, container creates a new instance for each and every HTTP session. So, if server has 20 active sessions, then container can have at most 20 individual instances of bean class. All HTTP requests within single session lifetime will have access to same single bean instance in that session scope.

Any state change to one instance, will not be visible to other instances. These instances are destructed as soon as the session is destroyed/end on server.

Example:

@Component
@Scope("session")
public class BeanClass {
}

5) Application scope:

This is similar to the singleton scope, but there is a very important difference with regards to the scope of the bean.

When beans are application scoped, the same instance of the bean is shared across multiple servlet-based applications running in the same ServletContext, while singleton scoped beans are scoped to a single application context only.

Example:

@Component
@Scope("application")
public class BeanClass {
}

5) websocket:

A single instance will be created and available during complete lifecycle of WebSocket.
Only valid in web-aware Spring ApplicationContext.

@Component
@Scope("websocket")
public class BeanClass {
}


Q) What is root application context in spring mvc ?  How it is loaded ?? Explain how incoming request mapped to a controller and mapped to a method??

ans) 

Before reading further, please understand that –

Spring can have multiple contexts at a time. One of them will be root context, and all other contexts will be child contexts.

All child contexts can access the beans defined in root context; but opposite is not true. Root context cannot access child contexts beans.

DispatcherServlet is essentially a Servlet (it extends HttpServlet) whose primary purpose is to handle incoming web requests matching the configured URL pattern. It take an incoming URI and find the right combination of controller and view. So it is the front controller.


ApplicationContext is loaded by ContextLoaderListener that contains beans that globally visible, like services, repositories, DataSource , etc. After the root application context is created, it’s stored in ServletContext as an attribute.

To get root application context in Spring controller, you can use WebApplicationContextUtils class.

@Autowired
ServletContext context; 
 
ApplicationContext ac = WebApplicationContextUtils.getWebApplicationContext(context);
 
if(ac == null){
    return "root application context is null";
}     


Q)  Difference between @RequestParam and @PathVariable??

Ans)

@RequestParam annotation used for accessing the query parameter values from the request. Look at the following request URL:

http://localhost:8080/springmvc/hello/101?param1=10&param2=20
In the above URL request, the values for param1 and param2 can be accessed as below:

public String getDetails(
    @RequestParam(value="param1", required=true) String param1,
        @RequestParam(value="param2", required=false) String param2){
...
}

@PathVariable

@PathVariable identifies the pattern that is used in the URI for the incoming request. Let’s look at the below request URL:

http://localhost:8080/springmvc/hello/101?param1=10&param2=20

@RequestMapping("/hello/{id}")    public String getDetails(@PathVariable(value="id") String id,
    @RequestParam(value="param1", required=true) String param1,
    @RequestParam(value="param2", required=false) String param2){
.......
}



Q)  In deep discussion on different annotation?

-> Each bean is registered with the ApplicationContext.

@Repository vs @Component vs @Service vs @Controller vs @RestController:

@Component: We can use @Component across the application to mark the beans as Spring's managed components.

@Respository: @Repository’s job is to catch persistence-specific exceptions and re-throw them as one of Spring’s unified unchecked exceptions.

@Service: We mark beans with @Service to indicate that they're holding the business logic. Besides being used in the service layer, there isn't any other special use for this annotation.

@Component (and @Service and @Repository) are used to auto-detect and auto-configure beans using classpath scanning.

@Bean is used to explicitly declare a single bean, rather than letting Spring do it automatically as above.

Sometimes automatic configuration is not an option. When? Let's imagine that you want to wire components from 3rd-party libraries (you don't have the source code so you can't annotate its classes with @Component), so automatic configuration is not possible.

The @Bean annotation returns an object that spring should register as bean in application context. The body of the method bears the logic responsible for creating the instance.


@Qualifier: Used to distinguish by name if two or more classes implements same interface. Used with autowiring.

internally uses ThreadPoolExecutor class

@SpringBootApplication = @Configuration + @AutoConfiguration + @ComponentScan

@AutoConfiguration =  the Spring Boot autoconfiguration represents a way to automatically configure a Spring application based on the dependencies that are present on the classpath.

@Value: used to load property and assign to variable from application.properties.
    
    Example:

            @Value(${server.port})
            Integer port;


Q) What is Concurrent Modification ?

Ans)

When one or more thread is iterating over the collection, in between, one thread changes the structure of the collection (either adding the element to the collection or by deleting the element in the collection or by updating the value at particular position in the collection) is known as Concurrent Modification

Q)  What is concurrent modification exception??

Ans)

The ConcurrentModificationException occurs when an object is tried to be modified concurrently when it is not permissible. This exception usually comes when one is working with Java Collection classes.

It is not permissible for a thread to modify a Collection when some other thread is iterating over it. This is because the result of the iteration becomes undefined with it. Some implementation of the Iterator class throws this exception

Example:

ArrayList<Integer> list = new ArrayList<>();  
  
        list.add(1);  
        list.add(2);  
        list.add(3);  
        list.add(4);  
        list.add(5);  
  
        Iterator<Integer> it = list.iterator();  
        while (it.hasNext()) {                   
Integer value = it.next();              
            System.out.println("List Value:" + value);  
            if (value.equals(3))  
                list.remove(value);  
        }  


  This message says that the exception is thrown when the next method is called as the iterator is iterating the list and we are making modifications in it simultaneously.

-> JDK 1.5 or higher provides with ConcurrentHashMap. These classes help us in avoiding concurrent modification exception.

Q) Difference between Fail Fast iterator and Fail Safe iterator ?

Ans)

Fail fast Iterator

Fail fast iterator while iterating through the collection , instantly throws Concurrent Modification Exception if there is structural modification  of the collection . Thus, in the face of concurrent modification, the iterator fails quickly and cleanly, rather than risking arbitrary, non-deterministic behavior at an undetermined time in the future.

Fail-fast iterator can throw ConcurrentModificationException in two scenarios :

Single Threaded Environment:
 
After the creation of the iterator , structure is modified at any time by any method other than iterator's own remove method.
  
Multiple Threaded Environment:

If one thread is modifying the structure of the collection while other thread is iterating over it .

The fail-fast behavior of an iterator cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. Fail-fast iterators throw ConcurrentModificationException on a best-effort basis. Therefore, it would be wrong to write a program that depended on this exception for its correctness: the fail-fast behavior of iterators should be used only to detect bugs.


Example of fail fast iterator is Map, HashMap.

Fail Safe Iterator :

Fail Safe Iterator makes copy of the internal data structure (object array) and iterates over the copied data structure.Any structural modification done to the iterator affects the copied data structure.  So , original data structure remains  structurally unchanged .Hence , no ConcurrentModificationException throws by the fail safe iterator.

Two  issues associated with Fail Safe Iterator are :

1. Overhead of maintaining the copied data structure i.e memory.

2.  Fail safe iterator does not guarantee that the data being read is the data currently in the original data structure.

Example of fail safe iterator is ConcurrentHashMap.

Q) How  Fail  Fast Iterator  come to know that the internal structure is modified ?

Ans) 

Iterator read internal data structure (object array) directly . The internal data structure(i.e object array) should not be modified while iterating through the collection. To ensure this it maintains an internal  flag "mods" .Iterator checks the "mods" flag whenever it gets the next value (using hasNext() method and next() method). Value of mods flag changes whenever there is an structural modification. Thus indicating iterator to throw ConcurrentModificationException.



Q)  Explain about internal working of hashmap?

Ans) 
-> hashCode() function  which returns an integer value is the Hash function. The important point to note that ,  this method is present in Object class 

-> hashCode method return  int value. So the Hash value is the int value returned by the hash function .

-> A bucket is used to store key value pairs . A bucket can have multiple key-value pairs . In hash map, bucket used simple linked list to store objects .

-> HashMap get(Key k) method calls hashCode method on the key object and applies returned hashValue to its own static hash function to find a bucket location(backing array) where keys and values are stored in form of a nested class called Entry (Map.Entry) . So you have concluded that from the previous line that Both key and value is stored in the bucket as a form of  Entry object .

-> Whenever we call get( Key k )  method on the HashMap object . First it checks that whether key is null or not .  Note that there can only be one null key in HashMap. 

-> If key is null , then Null keys always map to hash 0, thus index 0.

-> What if  when two different keys have the same hashcode ? This is also called hash collision.

-> The bucket is the linked list effectively . Its not a LinkedList as in a java.util.LinkedList - It's a separate (simpler) implementation just for the map . So we traverse through linked list , comparing keys in each entries using keys.equals() until it return true.  Then the corresponding entry object Value is returned .

-> In Map ,Any class(String etc.) can serve as a key if and only if it overrides the equals() and hashCode() method

-> In java 8, LinkedList is replaced by Binary Tree. Hash collisions have negative impact on the lookup time of HashMap. When multiple keys end up in the same bucket, then values along with their keys are placed in a linked list. In case of retrieval, linked list has to be traversed to get the entry. In worst case scenario, when all keys are mapped to the same bucket, the lookup time of HashMap increases from O(1) to O(n).


-> While converting the list to binary tree, hashcode is used as a branching variable. If there are two different hashcodes in the same bucket, one is considered bigger and goes to the right of the tree and other one to the left. But when both the hashcodes are equal, HashMap assumes that the keys are comparable, and compares the key to determine the direction so that some order can be maintained. It is a good practice to make the keys of HashMap comparable.

-> Above changes ensure performance of O(log(n)) in worst case scenarios (hash function is not distributing keys properly) and O(1) with proper hashCode().

-> This JDK 8 change applies only to HashMap, LinkedHashMap and ConcurrentHashMap.


Q)  Explain about internal working of Concurrent hashmap?

-> ConcurrentHashMap class is introduced in JDK 1.5

-> The underlined data structure for ConcurrentHashMap is Hashtable.

-> ConcurrentHashMap is an enhancement of HashMap as we know that while dealing with Threads in our application HashMap is not a good choice.

-> ConcurrentHashMap class is thread-safe i.e. multiple threads can operate on a single object.

-> At a time any number of threads are applicable for a read operation without locking the ConcurrentHashMap object which is not there in HashMap.

-> why need ConcurrentHashMap if we have HashTable ??

Hashtable provides concurrent access to the Map.Entries objects by locking the entire map to perform any sort of operation (update,delete,read,create). Suppose we have a web application , the overhead created by Hashtable  (locking the entire map) can be ignored under normal load. But under heavy load , the overhead of locking the entire map may prove fatal and may lead to delay response time and   overtaxing of the server.

-> In ConcurrentHashMap, the Object is divided into a number of segments according to the concurrency level.

-> The default concurrency-level of ConcurrentHashMap is 16.

-> In ConcurrentHashMap, at a time any number of threads can perform retrieval operation but for updated in the object, the thread must lock the particular segment in which the thread wants to operate. This type of locking mechanism is known as Segment locking or bucket locking. Hence at a time, 16 update operations can be performed by threads.

-> This indicates that 16 threads (number of threads equal to the concurrency level , which is by  default 16) can modify the collection at the same time , given ,each thread works on different bucket. So unlike hashtable, we perform any sort of operation ( update ,delete ,read ,create) without locking on entire map in ConcurrentHashMap.

-> when two objects lie in the same segment or same partition (entry), then parallel write would not be possible

-> Inserting null objects is not possible in ConcurrentHashMap as a key or value.

-> Why ConcurrentHashMap does not allow null keys and null values ?

Reason is that if map.get(key) returns null, you can't detect whether the key explicitly maps to null vs the key isn't mapped. In a non-concurrent map, you can check this via map.contains(key), but in a concurrent one, the map might have changed between 
calls.

-> Can multiple threads read from the Hashtable concurrently ?

No multiple threads can not read simultaneously from Hashtable. Reason, the get() method of  Hashtable is synchronized. As a result , at a time only one thread can access the get() method .
It is possible to achieve full  concurrency for reads (all the threads read at the same time) in  ConcurrentHashMap.

Example:

    Map<String, String> my_cmmap
            = new ConcurrentHashMap<String, String>();
 
        // Adding elements to the map
        // using put() method
        my_cmmap.put("1", "1");
        my_cmmap.put("2", "1");
        my_cmmap.put("3", "1");


        // Here we cant add Hello because 101 key
        // is already present in ConcurrentHashMap object
        my_cmmap.putIfAbsent(101, "Hello");                


        // Removing the mapping
        // with existing key 6
        // using remove() method
        String valueRemoved = my_cmmap.remove("3");  // prints "1" as removed value

        // Removing the mapping
        // with non-existing key 10
        // using remove() method
        valueRemoved = my_cmmap.remove("10"); // prints null, as key 10 doesnot exists

        // Now clear the map using clear()
        my_cmmap.clear();                  //Removes all key/values

        my_cmmap.get("2"); // prints 1 as value

        // Create an Iterator over the
        // ConcurrentHashMap
        Iterator<ConcurrentHashMap.Entry<Integer, String> >
            itr = my_cmmap.entrySet().iterator();
 
        // The hasNext() method is used to check if there is
        // a next element The next() method is used to
        // retrieve the next element
        while (itr.hasNext()) {
            ConcurrentHashMap.Entry<Integer, String> entry
                = itr.next();
            System.out.println("Key = " + entry.getKey()
                               + ", Value = "
                               + entry.getValue());
        }

-> Converting hasmap to ConcurrentHashMap.

    Map<String, String> myMap = getHashMap();
    ConcurrentMap<String, String> concurrentMap = new ConcurrentHashMap<> (myMap);


Q)  Explain about internal working of hashset?

-> All values of hashSet is unqiue, ie duplicate is not allowed.

    Example:

        HashSet<Object> hashset = new HashSet<Object>();
        hashset.add(3);
        hashset.add("Java Hungry");
        hashset.add("Blogspot");
        hashset.add(3);                     // duplicate elements
        hashset.add("Java Hungry");              // duplicate elements
        System.out.println("Set is "+hashset);

        Output: Set is [3, Java Hungry, Blogspot]

-> Now , what happens internally when you pass duplicate elements in the  add() method of the Set object , It will return false and do not add to the HashSet , as the element is already present

-> Internally HashSet uses HashMap to check if value already present or not.

        public boolean add(E e) {
            return map.put(e, PRESENT)==null;
       }

-> So , if map.put(key,value) returns null ,then
map.put(e, PRESENT)==null      will return true and element is added to the HashSet.

So , if map.put(key,value) returns old value of the key ,then
map.put(e, PRESENT)==null      will return false and element is  not added to the HashSet .

Q) Internal working of LinkedListHashMap ?

Ans)

-> Why do we need LinkedHashMap when we already had TreeMap and HashMap ?

HashMap provides constant time performance for basic operations like (add, remove and contains) method but elements are unordered.  In TreeMap elements are naturally sorted but there is increased cost associated with it. Performance of the basic operations (get, remove or put)  in TreeMap is slower in comparison i.e log(n) time.


->  How LinkedHashMap insertion order is maintained ?

The order is maintained based on the keys were inserted into the map .

-> What happens if we insert a key which is already present in the LinkedHashMap ? Does the order change ?

Insertion order of the LinkedHashMap does not get affected if a key is re-inserted into the LinkedHashMap object.
It first checks containsKey() method  before invoking put() method . If containsKey(K) returns true then the key is not added to the LinkedHashMap.

-> What is the time complexity of the add , remove and contains operations in LinkedHashMap ?

Time complexity of the add, remove and contains operations is constant time i.e O(1) .

-> Does LinkedHashMap Iterator behaves  like fail fast iterator or fail-safe iterator ?

LinkedHashMap iterator behaves like fail-fast iterator. As expected it will  throw ConcurrentModificationException.

Q)  How to remove duplicates from ArrayList? 

Ans)

    Using LinkedHashSet:

        Ex:

            // Create a new LinkedHashSet
            Set<T> set = new LinkedHashSet<>();
      
            // Add the elements to set
            set.addAll(list);
      
            // Clear the list
            list.clear();
      
            // add the elements of set
            // with no duplicates to the list
            list.addAll(set);

    Using streams java 8:

        Ex:

            list.stream().distinct().collect(Collectors.toList()); // distinct removes duplicate values



Q)  Write a program and create singleton class and related questions ??  And which is safe in multithreaded environment

Ans)

class Singleton {
    // Static variable reference of single_instance
    // of type Singleton
    private static Singleton single_instance = null;
 
    // Decl;aring a variable of type String
    public String s;
 
    // Constructor
    // Here we will be creating private constructor
    // restricted to this class itself
    private Singleton()
    {
        s = "Hello I am a string part of Singleton class";
    }
 
    // Static method
    // Static method to create instance of Singleton class
    public static Singleton getInstance()
    {
        if (single_instance == null)
            single_instance = new Singleton();
 
        return single_instance;
    }
}

-> Declare constructor as private and create getInstance() method to create object.

-> A singleton class itself is not thread safe. Multiple threads can access the singleton same time and create multiple objects.

How to make a singleton thread-safe ? 

-> A singleton can be made thread-safe by instantiating the singleton class inside a static inner class.

-> Multiple threads can access getInstance() and create objects same time, leading to multiple objects.

Ex:

    public class Singleton {
       private static class LoadSingleton {
          static final Singleton INSTANCE = new Singleton();
       }
       private Singleton() {}

       public static Singleton getInstance() {
          return LoadSingleton.INSTANCE;
       }
    }

-> If you are looking for a thread-safe version of singleton, the above code achieves it by implementing a static inner class. It does thread-safe lazy-initialization of the object without explicit synchronization. Note that the variable INSTANCE is wrapped in an inner class, utilizing the class loader to do synchronization. The class loader guarantees to complete all static initialization before it grants access to the class. This implementation lazy initializes the INSTANCE by calling LoadSingleton.INSTANCE when first accessed inside getInstance() method.

-> Spring's default scope is singleton. Spring singleton is not good for multithreading , for multithreading request scope should be used.

Q)  is it possible to use private constructor??  What is real life scenario of using it??

Ans)  Yes,  in singleton class.

Q) If parent is having singleton scope and child is having prototype scope and when i m getting parent bean , then how many instances will get created for child bean ?

Ans)

-> By default, Spring beans are singletons. The problem arises when we try to wire beans of different scopes. For example, a prototype bean into a singleton. This is known as the scoped bean injection problem.

-> This is the problem we face:

@Configuration
public class AppConfig {

    @Bean
    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)
    public PrototypeBean prototypeBean() {
        return new PrototypeBean();
    }

    @Bean
    public SingletonBean singletonBean() {
        return new SingletonBean();
    }
}

Notice that the first bean has a prototype scope, the other one is a singleton.


Now, let's inject the prototype-scoped bean into the singleton – and then expose if via the getPrototypeBean() method:

public class SingletonBean {

    // ..

    @Autowired
    private PrototypeBean prototypeBean;

    public SingletonBean() {
        logger.info("Singleton instance created");
    }

    public PrototypeBean getPrototypeBean() {
        logger.info(String.valueOf(LocalTime.now()));
        return prototypeBean;
    }
}

public static void main(String[] args) throws InterruptedException {
    AnnotationConfigApplicationContext context 
      = new AnnotationConfigApplicationContext(AppConfig.class);
    
    SingletonBean firstSingleton = context.getBean(SingletonBean.class);
    PrototypeBean firstPrototype = firstSingleton.getPrototypeBean();
    
    // get singleton bean instance one more time
    SingletonBean secondSingleton = context.getBean(SingletonBean.class);
    PrototypeBean secondPrototype = secondSingleton.getPrototypeBean();

    isTrue(firstPrototype.equals(secondPrototype), "The same instance should be returned");
}


-> To solve the problem is method injection with the @Lookup annotation:

@Component
public class SingletonLookupBean {

    @Lookup
    public PrototypeBean getPrototypeBean() {
        return null;
    }
}

Spring will override the getPrototypeBean() method annotated with @Lookup. It then registers the bean into the application context. Whenever we request the getPrototypeBean() method, it returns a new PrototypeBean instance.


Q)  Write a program and create immutable class and related questions

Ans)

To create an immutable class in Java, you have to do the following steps.

Declare the class as final so it can’t be extended.

Make all fields private so that direct access is not allowed.

Don’t provide setter methods for variables.

Make all mutable fields final so that its value can be assigned only once.

Initialize all the fields via a constructor performing deep copy.

Perform cloning of objects in the getter methods to return a copy rather than returning the actual object reference.

Ex:

final class Student {
    
    private final String name;
    private final int rollNo;

    Student(){
        this.name = name;
        this.rollNo = rollNo;
    }

    // Method 1
    public String getName() { return name; }
 
    // Method 2
    public int getRollNo() { return rollNo; }

}

Q)  Difference between JDBC and Hibernate??

JDBC:

-> In JDBC, one needs to write code to map the object model’s data representation to the schema of the relational model.

-> JDBC enables developers to create queries and update data to a relational database using the Structured Query Language (SQL).

-> JDBC code needs to be written in a try catch block as it throws checked exception(SQLexception).

-> JDBC is database dependent i.e. one needs to write different codes for different database.

-> Creating associations between relations is quite hard in JDBC.

Hibernate:

-> Hibernate maps the object model’s data to the schema of the database itself with the help of annotations. (ORM)

-> Hibernate uses HQL (Hibernate Query Language) which is similar to SQL but understands object-oriented concepts like inheritance, association etc.

-> Whereas Hibernate manages the exceptions itself by marking them as unchecked.

-> Whereas Hibernate is database independent and same code can work for many databases with minor changes.

-> Associations like one-to-one, one-to-many, many-to-one, and many-to-many can be acquired easily with the help of annotations.

Q)  Explain hibernate architecture?

Ans)

-> Configuration: Like database connection information, create transaction managers etc.

-> SessionFactory: SessionFactory is an Interface which is present in org.hibernate package and it is used to create Session Object.  It is immutable and thread-safe in nature.

-> Session: Session object is created based upon SessionFactory object i.e. factory. It opens the Connection/Session with Database software through Hibernate Framework. Session object is used to perform CRUD operations.

    Session session=factory.buildSession();

-> Transaction: Create by Session object. 

    Transaction tx=session.beginTransaction();
    tx.commit();

-> Query: A Query instance is obtained by calling Session.createQuery(). Has some additional functionalities by calling setMaxResults(), setFirstResult().

    Query query=session.createQuery()

-> Creteria: Criteria is a simplified API for retrieving entities by composing Criterion objects.
    
    Criteria criteria=session.createCriteria();

-> Flow of data:

Suppose We want to insert or retrive an Object to the database. 

Stage I: In first stage, we will write the persistence logic to perform some specific operations to the database with the help of Hibernate Configuration file.

Stage II:In second stage, our class (ie our repository class) which contains the persistence logic will interact with the hibernate framework where hibernate framework gives some abstraction do perform some task. Now here the picture of java class is over. Now Hibernate is responsible to perform the persistence logic with the help of layers which is below of Hibernate framework or we can say that the layers which are the internal implementation of Hibernate.

Stage III:In third stage, our hibernate framework interact which JDBC, JTA etc to go to the database to perform that persistence logic.

Stage IV & V:In fourth & fifth stage, hibernate is interact with Database with the help of JDBC driver. Now here hibernate perform that persistence logic which is nothing but CRUD operation. 

Q)  How to deal with database deadlock situation??

Ans)

    Can be prevented by ISOLATION level of @Transactional by acquiring lock. Either SERIALIZABLE will be the best solution but it will increase delay as it locks full table.

Q)  Explain database Sharding ??

Ans)
    
    Sharding which is also known as data partitioning works on the same concept of sharing the Pizza slices. It is basically a database architecture pattern in which we split a large dataset into smaller chunks.

    Whenever any application starts receiving a huge amount of concurrent requests and it sees significant growth of the users on the website it eventually needs to scale to handle the increasing amount of data or traffic on the website.So we need to scale our database dynamically and database sharding is the technique that can fulfill this job.

Q)  Advantages of microservices over other?

Ans)

    Improved fault isolation: Larger applications can remain mostly unaffected by the failure of a single module

    Different programming language can be used to make different microservices.

    Small team members can work on them.

    More scalable , as microservices can be deploy on different servers / data centers.

    More reusable.

Q)  What all things to consider before creating microservice ?

Ans)

    1) Single Responsibility Principal

    A microservice should have single responsibility so that it will be easy to maintain and be reusable. When the microservice has a single responsibility, we can easily choose the best way to implement it to accomplish that particular responsibility.
    Which also means each microservice maintains its own database and no other service should access the other service's database directly.

    2) Programming frameworks

    Microservices are exposed using an API so the underlying programming framework used, may be different for different microservices. It is definitely great flexibility but if we are not cautious then we may end up using too many frameworks and it might be unmanageable as it increases deployment cost by having to write and maintain different scrips for deploying different frameworks. So choose only a couple of programming frameworks which suits your needs.

    3) Secrets Management
    
    Microservices might require access to various resources like databases, file servers, and third-party services. Credentials are required to access these resources and how are we going to manage these credentials? Copying credentials in each microservice pose a serious security risk and also when we rotate credentials, we need to change in many services. It is recommended to create a few microservice to manage and distribute these secrets other services.

    4) Dependency graph
    
    Understanding how each microservices depend on each other is important to achieve better performance and reliability. 

    5) Versioning
    
    Microservices talk to each other using API and overtime API will change and it may not be possible to make changes to all the dependent microservices at once and we need support backward compatibility so make sure you have thought about versioning of API from the beginning.

    6) Discovery
    
    Service endpoints of each microservice should easily discoverable and reachable by other microservices. Using qualified domain names as service endpoints instead of hostnames or IP addresses is recommended, as hostnames and IPs might change. 


Q)  What is best way to manage transaction when multiple microservice involved??

Ans)
    It is recommended each microservice maintains its own database and no other service should access the other service's database directly.

    We need coordinator, which is the central place which stores each microservice's status of the transaction. Then if any microservice fails to fullfill the job it will be infomed to coordinator, which will start roll back of the transaction by informing each committed microservice.

    Then dpending upon requirment we can choose from 2 phase commit, Saga pattern etc.

Q)  SOA vs microservices ??

Ans)

    SOA applications are built to perform numerous business tasks, but microservices are built to perform a single business task.

Q)  Difference between iterator and splitIterator ?

Ans) 
    Iterator:

    -> Introduced in Java 1.2
    -> Iterator only iterates elements individually
    -> It is an iterator for whole collection API

    splitIterator:

    -> The Interface Spliterator is included in JDK 8.
    -> Spliterator traverse elements individually as well as in bulk.
    -> It is an iterator for both Collection and Stream API, except Map implementation classes 

    The main functionalities of Spliterator are: 

    Splitting the source data
    Processing the source data

    Ex:

    // Create an object of array list
        ArrayList<Integer> list = new ArrayList<>();
  
        // Add elements to the array list
        list.add(101);
        list.add(201);
        list.add(301);
        list.add(401);
        list.add(501);
  
        // create a stream on the list
        Stream<Integer> str = list.stream();

        // Get Spliterator object on stream
        Spliterator<Integer> splitr = str.spliterator();
  
        // Print getExactSizeIfKnown
        // returns exact size if finite
        // or return -1
        System.out.println("Exact size: "
                           + splitr.getExactSizeIfKnown()); // Exact size: 5
  
  
        System.out.println("Elements of ArrayList :");
  
        // print elements using forEachRemaining
        splitr.forEachRemaining(
            (n) -> System.out.println(n));              // prints all elemnts in list
  
        // Obtaining another Stream to the array list.
        Stream<Integer> str1 = list.stream();
  
        splitr = str1.spliterator();
  
        // Obtain spliterator using     trySplit() method
        Spliterator<Integer> splitr2 = splitr.trySplit();
  
        // If splitr can be partitioned use splitr2 first.
        if (splitr2 != null) {
  
            System.out.println("Output from splitr2: ");
            splitr2.forEachRemaining(
                (n) -> System.out.println(n));           // 101 and 202
        }
  
        // Now, use the splitr
        System.out.println("Output from splitr1: ");
        splitr.forEachRemaining(
            (n) -> System.out.println(n));                // 301,401 and 501


Q)  Can a constructor is synchronized in java?

Ans)

    No, a constructor cannot be synchronized in Java. The JVM ensures that only one thread can invoke a constructor call at a given point in time. That is why no need to declare a constructor as synchronized and it is illegal in Java. However, we can use synchronized blocks inside a constructor.

    If we are trying to put a synchronized keyword before a constructor, the compiler says that "error: modifier synchronized not allowed here".
    

Q)  In which scenario we should use static block?

Ans)

    The purpose of using a static initialization block is to write that logic inside static block that is executed during the class loading.
    It is mostly used for changing default value of static variables.

Q)  need of marker interface?

    Looking carefully on marker interface in Java e.g. Serializable, Clonnable and Remote it looks they are used to indicate something to compiler or JVM. So if JVM sees a Class is Serializable it done some special operation on it, similar way if JVM sees one Class is implement Clonnable it performs some operation to support cloning.

    So in short Marker interface indicate, signal or a command to Compiler or JVM.

Q)  Need of serialversionid??

Ans)

    SerialVersionUID is a unique identifier for each class, JVM uses it to compare the versions of the class ensuring that the same class was used during Serialization is loaded during Deserialization.
    
    SerialVersionUID is generated based on a class name, implemented interfaces, and all public and protected members

    New SerialVersionUID is generated when class is modified.

    Specifying one gives more control, though JVM does generate one if you don't specify. 

    If you ignore them for now, and find later that you need to change the class in some way but maintain compatibility w/ old version of the class, you can use the JDK tool "serialver" to generate the serialVersionUID on the old class, and explicitly set that on the new class. Do not forget to to implement readObject() and writeObject() methods.

    In case if the newer version changes any public member to protected, the default SerializableVersionUID will be different and will raise an InvalidClassExceptions.

    A serializable class can declare its own serialVersionUID explicitly by declaring a field named “serialVersionUID” that must be static, final, and of type long.

    private static final long serialVersionUID = 4L;

    Here the serialVersionUID represents the class version, and we should increment it if the current version of your class is modified such that it is no longer backwards compatible with its previous version.

    Use transient keyword if we donot want to serialize attribute.
    //Not able to serialize
    transient private String transientVariable = "this is a transient instance field";

Q)  Features / advantages of spring boot??

Ans)

    1) AutoConfiguration:

    It detects the presence of certain a Class in the Classpath and then automatically configures it for you.

    2) Starter POMs:

    By using Spring Boot Starter POMs or starter dependency feature, you can get all of these by just adding spring-boot-starter-web dependency in your pom.xml rather than adding many dependencies individually.

    3) Actuator:

    The actuator is another awesome feature of Spring Boot that allows seeing what's going on inside a running Spring Boot application. With all its goodness of auto-configuration, there comes a risk of not knowing what is inside your application and that risk is addressed by the Spring Actuator.

    It provides a lot of insight and metrics about running applications in production. For example, by using Actuator, you can find out exactly which beans are configured in the Application context, what are auto-configuration decisions made, what environment variables, system properties, command line arguments are available to an application, and many more.

    You can even get a trace of HTTP requests handled by the application, along with various useful application metrics, e.g. CPU and Memory usage, garbage collection details, web requests, and data source usage.

    Spring Boot Actuator also provides several endpoints to retrieve this data, e.g. you can get all this using RESTful APIs.

    You also need to secure access to Actuator endpoints because it not only exposes confidential information but also it's dangerous. For example, anyone can stop your application by using /shutdown endpoints.

    Like any other Spring application, you can use Spring Security to protect Actuator endpoints. 

    4) Spring Boot Initializer

    All you need to provide is the Project MetaData in GUI, e.g. name of your project, Group, Artifact, etc. It also allows you to choose a starter dependency from a big list, e.g. web, JPA, or security starter.


Q)  How do you define global variable in bean implementation?

Ans) Use static variables inside class.
    
    @Component
    public class Globals {
       public static int globalInt = 0;
       ///
    }

    Then use it like,

    class A{

    @Autowired
    Globals globals;

        public void m1(){
            System.out.println(globals.globalInt);   //Static variables are called by class name but static methods cannot be called by class name. we can directly call static methods without classname
        }
    }

    Or using application.properties.

    application.properties

    myConfig.configA:A


    then in code,

    @Value("${myConfig.configA}")
    private String configA;

Q)  why non-static variables are not allowed in static block ??

Ans)

    Before getting into the error, lets first understand what each of the methods means: 
 

    Static Method: A static method is a method that belongs to a class, but it does not belong to an instance of that class and this method can be called without the instance or object of that class. In the static method, the method can only access only static data members and static methods of another class or same class but cannot access non-static methods and variables.

    Non-static method: Any method whose definition doesn’t contain the static keyword is a non-static method. In the non-static method, the method can access static data members and static methods as well as non-static members and method of another class or same class, also can change the values of any static data member.

    Static variable initialized when class is loaded into JVM on the other hand instance variable has a different value for each instance and they get created when an instance of an object is created either by using the new() operator


Q)  What are idempotent methods??

Ans) 

    In the context of REST APIs, when making multiple identical requests has the same effect as making a single request – then that REST API is called idempotent. The following HTTP methods are idempotent: GET, HEAD, OPTIONS, TRACE, PUT and DELETE.

    These are HTTP methods that don't change the resource on the server-side. For example, using a GET or a HEAD request on a resource URL should NEVER change the resource

    Idempotency is an important thing while building a fault-tolerant RESTful API. Idempotency is also the reason why should you use PUT over POST to update a resource in REST.


Q)  What different http  methods and which to use in which scenario??

Ans)

    1) GET is both Safe and Idempotent.
    2) HEAD is also both safe and idempotent.
    3) OPTIONS is also safe and idempotent.
    4) PUT is not safe but idempotent.
    5) DELETE is not safe but idempotent.
    6) POST is neither safe nor idempotent.
    7) PATCH is also neither safe nor idempotent.

    While both PUT and POST can be used to create resources. The POST method call will create a child resource. The PUT method call will either create a new resource or update an existing one.

    Another important difference between the methods is that PUT is an idempotent method while POST is not. For instance, calling the PUT method multiple times will either create or update the same resource. On the contrary, multiple POST requests will lead to the creation of the same resource multiple times.

    Ex:

        @PostMapping("/addresses")
        Address createNewAddress(@RequestBody Address newAddress) {
            return repository.save(newAddress);
        }

        @PutMapping("/addresses/{id}")
        Address replaceEmployee(@RequestBody Address newAddress, @PathVariable Long id) {

            return repository.findById(id)
                .map(address -> {
                    address.setCity(newAddress.getCity());
                    address.setPin(newAddress.getPostalCode());
                    return repository.save(address);
                })
                .orElseGet(() -> {
                    return repository.save(newAddress);
                });
        }

    Note : Maintaining Idempotency is the duty of API maker.Just putting Delete method do not give Idempotency.

    Delete Request example:

        @RequestMapping(value = "/users/{id}", method = RequestMethod.DELETE)
         public void deleteUser(@PathVariable String id) {
              userService.deleteUser(id);
        }

        public void deleteUser(String id) {
             users.removeIf(u -> u.getId().equals(id));     
        }

Q)  Write api endpoint to upload a file and download file example ??

Ans)
    -> In applications.properties

    ## MULTIPART (MultipartProperties)
    # Enable multipart uploads
    spring.servlet.multipart.enabled=true
    # Threshold after which files are written to disk.
    spring.servlet.multipart.file-size-threshold=2KB
    # Max file size.
    spring.servlet.multipart.max-file-size=200MB
    # Max Request Size
    spring.servlet.multipart.max-request-size=215MB

    ## File Storage Properties
    # All files uploaded through the REST API will be stored in this directory
    file.upload-dir=/Users/callicoder/uploads
    

    -> Spring Boot has an awesome feature called @ConfigurationProperties using which you can automatically bind the properties defined in the application.properties file to a POJO class.

    @ConfigurationProperties(prefix = "file")
    public class FileStorageProperties {
        private String uploadDir;

        public String getUploadDir() {
            return uploadDir;
        }

        public void setUploadDir(String uploadDir) {
            this.uploadDir = uploadDir;
        }
    }
    
    The @ConfigurationProperties(prefix = "file") annotation does its job on application startup and binds all the properties with prefix file to the corresponding fields of the POJO class.

    -> Now, To enable the ConfigurationProperties feature, you need to add @EnableConfigurationProperties annotation to any configuration class.

internally uses ThreadPoolExecutor class

        @SpringBootApplication
        @EnableConfigurationProperties({
                FileStorageProperties.class
        })
        public class FileDemoApplication {

            public static void main(String[] args) {
                SpringApplication.run(FileDemoApplication.class, args);
            }
        }

    -> Create response POJO in com.example.payload package.

        public class UploadFileResponse {
            private String fileName;
            private String fileDownloadUri;
            private String fileType;
            private long size;

            public UploadFileResponse(String fileName, String fileDownloadUri, String fileType, long size) {
                this.fileName = fileName;
                this.fileDownloadUri = fileDownloadUri;
                this.fileType = fileType;
                this.size = size;
            }

            // Getters and Setters (Omitted for brevity)
        }

    -> Create service class,

        @Service
        public class FileStorageService {

            private final Path fileStorageLocation;

            @Autowired
            public FileStorageService(FileStorageProperties fileStorageProperties) {
                this.fileStorageLocation = Paths.get(fileStorageProperties.getUploadDir())
                        .toAbsolutePath().normalize();

                try {
                    Files.createDirectories(this.fileStorageLocation);
                } catch (Exception ex) {
                    throw new FileStorageException("Could not create the directory where the uploaded files will be stored.", ex);
                }
            }

            public String storeFile(MultipartFile file) {
                // Normalize file name
                String fileName = StringUtils.cleanPath(file.getOriginalFilename());

                try {
                    // Check if the file's name contains invalid characters
                    if(fileName.contains("..")) {
                        throw new FileStorageException("Sorry! Filename contains invalid path sequence " + fileName);
                    }

                    // Copy file to the target location (Replacing existing file with the same name)
                    Path targetLocation = this.fileStorageLocation.resolve(fileName);
                    Files.copy(file.getInputStream(), targetLocation, StandardCopyOption.REPLACE_EXISTING);

                    return fileName;
                } catch (IOException ex) {
                    throw new FileStorageException("Could not store file " + fileName + ". Please try again!", ex);
                }
            }

            public Resource loadFileAsResource(String fileName) {
                try {
                    Path filePath = this.fileStorageLocation.resolve(fileName).normalize();
                    Resource resource = new UrlResource(filePath.toUri());
                    if(resource.exists()) {
                        return resource;
                    } else {
                        throw new MyFileNotFoundException("File not found " + fileName);
                    }
                } catch (MalformedURLException ex) {
                    throw new MyFileNotFoundException("File not found " + fileName, ex);
                }
            }
        }

    -> Create rest controller,

        @RestController
        public class FileController {

            private static final Logger logger = LoggerFactory.getLogger(FileController.class);

            @Autowired
            private FileStorageService fileStorageService;
            
            @PostMapping("/uploadFile")
            public UploadFileResponse uploadFile(@RequestParam("file") MultipartFile file) {
                String fileName = fileStorageService.storeFile(file);

                String fileDownloadUri = ServletUriComponentsBuilder.fromCurrentContextPath()
                        .path("/downloadFile/")
                        .path(fileName)
                        .toUriString();

                return new UploadFileResponse(fileName, fileDownloadUri,
                        file.getContentType(), file.getSize());
            }

            @PostMapping("/uploadMultipleFiles")
            public List<UploadFileResponse> uploadMultipleFiles(@RequestParam("files") MultipartFile[] files) {
                return Arrays.asList(files)
                        .stream()
                        .map(file -> uploadFile(file))
                        .collect(Collectors.toList());
            }

            @GetMapping("/downloadFile/{fileName:.+}")
            public ResponseEntity<Resource> downloadFile(@PathVariable String fileName, HttpServletRequest request) {
                // Load file as Resource
                Resource resource = fileStorageService.loadFileAsResource(fileName);

                // Try to determine file's content type
                String contentType = null;
                try {
                    contentType = request.getServletContext().getMimeType(resource.getFile().getAbsolutePath());
                } catch (IOException ex) {
                    logger.info("Could not determine file type.");
                }

                // Fallback to the default content type if type could not be determined
                if(contentType == null) {
                    contentType = "application/octet-stream";
                }

                return ResponseEntity.ok()
                        .contentType(MediaType.parseMediaType(contentType))
                        .header(HttpHeaders.CONTENT_DISPOSITION, "attachment; filename=\"" + resource.getFilename() + "\"")
                        .body(resource);
            }
        }

    -> to generate PDF file use ITextPDF.

Q)  Different http codes like 404, 401, 500 etc

-> HTTP Status Code 503 - Service Unavailable
    401 - Unauthorized 
    200 - Ok, Success
    201 – This is the status code that confirms that the request was successful and, as a result, a new resource was created. Typically, this is the status code that is sent after a POST/PUT request.
    204 – This status code confirms that the server has fulfilled the request but does not need to return information. Examples of this status code include delete requests or if a request was sent via a form and the response should not cause the form to be refreshed or for a new page to load.
    400 – The server cannot understand and process a request due to a client error. Missing data, domain validation, and invalid formatting are some examples that cause the status code 400 to be sent.
    410 – Resource requested is no longer available and will not be available again.

Q) Which code review tools you use ?

Ans) SonarQube, 


Q)  process of configuring actuator?? And what are some important endpoints

Ans)

    -> Most applications exposes endpoints via HTTP, where the ID of the endpoint along with a prefix of /actuator is mapped to a URL. For example, by default, the health endpoint is mapped to /actuator/health.

    By default, only /health and /info are exposed via Web APIs. 

    Use management.endpoints.web.exposure.include=* to expose all endpoints through the Web APIs.

    application.properties

    management.endpoints.web.exposure.include=*
 
    # To expose only selected endpoints
    #management.endpoints.jmx.exposure.include=health,info,env,beans

    Some endpoints are: 

    /beans - Returns a complete list of all the Spring beans in your application.
    /mappings - Displays a collated list of all @RequestMapping paths
    /env  -  Returns list of properties in current environment
    /health - Returns application health information.  It return status UP.
    /loggers  -  The configuration of loggers in the application
    /shutdown -  Lets the application be gracefully shutdown. Disabled by default.
    /metrics - It shows several useful metrics information like JVM memory used, system CPU usage, open files, 

Q) CORS ??

Ans)

    Cross-Origin Resource Sharing (CORS) is an HTTP-header based mechanism that allows a server to indicate any origins (domain, scheme, or port) other than its own from which a browser should permit loading resources


Q)  Different types of Joins?

Ans)
    INNER JOIN
    Chances are, you've already written a statement that uses a MySQL INNER JOIN. It is the most common type of join. MySQL INNER JOINS return all rows from multiple tables where the join condition is met. Returns rows from both tables where condition matches.

    SELECT columns
    FROM table1 
    INNER JOIN table2
    ON table1.column = table2.column;

    LEFT OUTER JOIN
    Another type of join is called a MySQL LEFT OUTER JOIN. This type of join returns all rows from the LEFT-hand table specified in the ON condition and only those rows from the other table where the joined fields are equal

    RIGHT OUTER JOIN:
    Another type of join is called a MySQL RIGHT OUTER JOIN. This type of join returns all rows from the RIGHT-hand table specified in the ON condition and only those rows from the other table where the joined fields are equal



Q)  Store procedure?

Ans)
    
    Code reusability.
    
    Lesser Network transfer – E.g. for web applications -instead of calling individual queries, a procedure that can directly return the desired result can be executed.

    Syntax:

    DELIMITER $$
    CREATE PROCEDURE GetStudentData()
    BEGIN
        SELECT * FROM studentMarks;
    END$$
    DELIMITER ;

    Ex:

        DELIMITER //
        CREATE PROCEDURE stored_proc_tutorial.spGetDetailsByStudentName(IN studentId INT)
        BEGIN
            SELECT * FROM studentMarks where stud_id = studentId;
        END //
        DELIMITER ;

Q) Indexing in MYSQL ??

Ans)

    Basically an index is a map of all your keys that is sorted in order. With a list in order, then instead of checking every key, it can do something like this:

    1: Go to middle of list - is higher or lower than what I'm looking for?

    2: If higher, go to halfway point between middle and bottom, if lower, middle and top

    3: Is higher or lower? Jump to middle point again, etc.

    Using that logic, you can find an element in a sorted list in about 7 steps, instead of checking every item.

    We can create an index by using one or more columns of the table for efficient access to the records.

    When a table is created with a primary key or unique key, it automatically creates a special index named PRIMARY. We called this index as a clustered index. All indexes other than PRIMARY indexes are known as a non-clustered index or secondary index.

    Suppose we have a contact book that contains names and mobile numbers of the user. In this contact book, we want to find the mobile number of Martin Williamson. If the contact book is an unordered format means the name of the contact book is not sorted alphabetically, we need to go over all pages and read every name until we will not find the desired name that we are looking for. This type of searching name is known as sequential searching.

    To find the name and contact of the user from table contactbooks, generally, we used to execute the following query:

    SELECT mobile_number FROM contactbooks WHERE first_name = 'Martin' AND last_name = 'Taybu'; 

    This query is very simple and easy. Although it finds the phone number and name of the user fast, the database searches entire rows of the table until it will not find the rows that you want. Assume, the contactbooks table contains millions of rows, then, without an index, the data retrieval takes a lot of time to find the result. In that case, the database indexing plays an important role in returning the desired result and improves the overall performance of the query.

Q)  What are triggers??  How many triggers can be applied to table??

Ans)
    
    It is a special type of stored procedure that is invoked automatically in response to an event. Each trigger is associated with a table, which is activated on any DML statement such as INSERT, UPDATE, or DELETE.

    A trigger is called a special procedure because it cannot be called directly like a stored procedure. The main difference between the trigger and procedure is that a trigger is called automatically when a data modification event is made against a table.

    We can define the maximum six types of actions or events in the form of triggers: Before insert, After insert,Before update, After update, Before Delete, After Delete

Q)  Different types of resultsets in jdbc??

Ans) 2 types of resultSet are there in JDBC: forward and BiDirectional
    
      1) You can move the cursor of the forward only ResultSets using the next() method of the ResultSet interface
      
      ResultSet rs = stmt.executeQuery("select * from Dataset");
      System.out.println("Contents of the table");

      while(rs.next()) {
         System.out.print("Brand: "+rs.getString("Mobile_Brand")+", ");
         System.out.print("Sale: "+rs.getString("Unit_Sale"));
         System.out.println("");
      }
    
      2) Bidirectional ResultSet: A bi-directional ResultSet object is the one whose cursor moves in both forward and backward directions.

      //Creating a Statement object
      Statement stmt = con.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE,
      ResultSet.CONCUR_UPDATABLE);
      //Retrieving the data
      ResultSet rs = stmt.executeQuery("select * from Dataset");
      rs.afterLast();

      while(rs.previous()) {
         System.out.print("Brand: "+rs.getString("Mobile_Brand")+", ");
         System.out.print("Sale: "+rs.getString("Unit_Sale"));
         System.out.println("");
      }

      -> You cannot pass a Row Set object over the network.
      -> A ResultSet always maintains the connection with the database.


Q)  What is rowset?

-> You can pass a Row Set object over the network.
-> Row Set interface extends ResultSet interface in java.
-> A Row Set can be connected, disconnected from the database.

    // Creating a RowSet
            JdbcRowSetrowSet = RowSetProvider.newFactory()
                                   .createJdbcRowSet();
 
            // Setting URL, username, password
            rowSet.setUrl(
                "jdbc:oracle:thin:@localhost:1521:xe");
            rowSet.setUsername("root");
            rowSet.setPassword("pass");
 
            // Creating a query
            rowSet.setCommand("select * from Student");
 
            // Executing the query
            rowSet.execute();
 
            // Processign the results
            while (rowSet.next()) {
 
            }



Q)  thread local??

Ans)

    -> You create a ThreadLocal instance just like you create any other Java object - via the new operator. Here is an example that shows how to create a ThreadLocal variable:

        private ThreadLocal threadLocal = new ThreadLocal();
    
    This only needs to be done once per thread. Multiple threads can now get and set values inside this ThreadLocal, and each thread will only see the value it set itself.

    threadLocal.set("A thread local value");  //Set value

    String threadLocalValue = (String) threadLocal.get(); // Get value
    threadLocal.remove(); // Remove value

    Ex:

        public static void main(String[] args) {
            MyRunnable sharedRunnableInstance = new MyRunnable();

            Thread thread1 = new Thread(sharedRunnableInstance);
            Thread thread2 = new Thread(sharedRunnableInstance);

            thread1.start();
            thread2.start();

            thread1.join(); //wait for thread 1 to terminate
            thread2.join(); //wait for thread 2 to terminate
        }

        public class MyRunnable implements Runnable {

            private ThreadLocal<Integer> threadLocal = new ThreadLocal<Integer>();

            @Override
            public void run() {
                threadLocal.set( (int) (Math.random() * 100D) );

                try {
                    Thread.sleep(2000);
                } catch (InterruptedException e) {
                }

                System.out.println(threadLocal.get());
            }
        }

        This example creates a single MyRunnable instance which is passed to two different threads. Both threads execute the run() method, and thus sets different values on the ThreadLocal instance. If the access to the set() call had been synchronized, and it had not been a ThreadLocal object, the second thread would have overridden the value set by the first thread.

        However, since it is a ThreadLocal object then the two threads cannot see each other's values. Thus, they set and get different values.

Q)  how to call 4 rest service at the same time where response time is maximum?

Ans)

    -> use @Asyn and return type as CompletableFuture.
    -> when we annotate a method of a bean @Async annotation, Spring will execute it in a separate thread and the caller of the method will not wait till the method is completed execution.
    -> The AsyncConfigurationfile allows us to use the use the asynchronous functions and the @Async annotation.

        @Configuration
        @EnableAsync
        public class AsyncConfiguration  {
            @Bean
            public Executor asyncExecutor() {
                ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
                return executor;
            }
        }

    https://medium.com/sipios/how-to-make-parallel-calls-in-java-springboot-application-and-how-to-test-them-dcc27318a0cf


Q)  How to generate custom ID in hibernate?

Ans)

    @Entity
    @Table(name = "employee_customid")
    public class Employee implements Serializable {
        
        @Id
        @GenericGenerator(name = "string_based_custom_sequence", strategy = 
        "com.ie.customid.EmployeeIdGenerator")
        @GeneratedValue(generator = "string_based_custom_sequence")
        @Column(name = "custom_emp_id")
        private String id;
    
    }

    public class EmployeeIdGenerator implements IdentifierGenerator {
        
        public Serializable generate(SessionImplementor session, Object object) throws HibernateException {
            
            return "abc"+Math.random();
        }

    }

Q) Program to generate unique id ??

Ans)
     import java.util.UUID;

     public static String generateString() {
        String uuid = UUID.randomUUID().toString();
        return "uuid = " + uuid;
    }

Q)  Concurrent map vs synchronized map? 

Ans)
    
    Synchronized HashMap：

    Each method is synchronized using an object level lock. So the get and put methods on synchMap acquire a lock.

    Locking the entire collection is a performance overhead. While one thread holds on to the lock, no other thread can use the collection.
    
    ConcurrentHashMap was introduced in JDK 5.

    There is no locking at the object level,The locking is at a much finer granularity. For a ConcurrentHashMap, the locks may be at a hashmap bucket level.

    The effect of lower level locking is that you can have concurrent readers and writers which is not possible for synchronized collections. This leads to much more scalability.

Q)  application server vs web server??

Ans)    
    The main difference between Web server and application server is that web server is meant to serve static pages e.g. HTML and CSS, while Application Server is responsible for generating dynamic content by executing server side code e.g. JSP, Servlet or EJB.

    Application server example:  Apache Tomcat, gGlassfish, JBoss
    Web server: NGINX, Solaris 

Q)  How to disable tomcat from spring boot and add another server using profiling??

Ans)
internally uses ThreadPoolExecutor class

        @SpringBootApplication(exclude = {EmbeddedServletContainerAutoConfiguration.class, 
                                  WebMvcAutoConfiguration.class})

        to exclude Spring Boot's auto-configuration for embedded servlet containers. Additionally, you need to set the following property for the non-REST cases, so that Spring Boot won't try to start a WebApplicationContext (which needs a servlet container):

        application.propeties:

        spring.main.web-environment=false

        Then enable the embedded Tomcat in your REST profile by importing EmbeddedServletContainerAutoConfiguration.class (this delays the autoconfiguration until after the REST profile has been loaded:

        @Profile({"REST"})
        @Configuration
        @Import(EmbeddedServletContainerAutoConfiguration.class)   // Import any other server
        public class HttpConfiguration {
            // ...
        }

Q)  Profile in spring boot ?

Ans)

    we need to create three  application.properties:

     application-dev.properties 
     application-test.properties 
     application-prod.properties 

     Of course, the application.properties will remain as a master properties file, but if we override any key in the profile-specific file, the latter will gain precedence.

     @Configuration
     class appConfig{

        @Profile("dev")
        @Bean
        public String devDBConnectionBean(){
            return "dev";
        }

     }

Q)  how to control multiple request coming from clients and how to control access to shared resource??  

ans) semaphore

Q)  Immutable vs final ??

ans) final String name = "John";

    When you write the above your code is telling the compiler that the reference name will always point to the same memory location. Now why I say memory location because it might happen that the object the reference is pointing to is mutable like array or list of integers. So if I say final int[] arr = {5,6,1}; I can do arr[2] = 3; but I can't do arr = {3,4,5} cause you will be trying to assign a new int[] to final variable arr which is a new memory location and seeing this compiler will show error.

    String name = "John";
    name = "Sam"; 
    
    Above the name variable of type String is immutable because String in java is immutable which means you can't change the state of the object pointed out by the reference name once it is created and even if you change it to Sam, it won't change.


Q)  print first non repeating characters of string??

ans)    The primitive and object versions of data types (i.e. int and Integer, double and Double, etc.) are not really compatible with each other in Java. They are made compatible through the extra step of auto-boxing/unboxing. Thus, if you have a stream of primitive ints and if you try to use the object versions of Stream and Function (i.e. Stream<Integer> and Function<Integer, Integer>), you will incur the cost of boxing and unboxing the elements.

To eliminate this problem, the function package contains primitive specialized versions of streams as well as functional interfaces. For example, instead of using Stream<Integer>, you should use IntStream. You can now process each element of the stream using IntFunction. This will avoid auto-boxing/unboxing altogether.

Thus, whenever you want to process streams of primitive elements, you should use the primitive specialized streams (i.e. IntStream, LongStream, and DoubleStream) and primitive specialized functional interfaces (i.e. IntFunction, IntConsumer, IntSupplier, etc.) to achieve better performance.
        
        Ex:

                        Stream<Object> stream1 = Stream.of(1, 2, 3); //Will compile fine
                        IntStream intStream1 = IntStream.of(4, 5, 6); //Will compile fine

                        Stream<Object> stream2 = IntStream.of(4, 5, 6); //Does not compile
                        Stream<Object> stream3 = IntStream.of(4, 5, 6).mapToObj(e -> e); //mapToObj method is needed
                        IntStream intStream2 = Stream.of(4, 5, 6).mapToInt(e -> e); //mapToInt method is needed
        
        Other example: 

            Stream<String> stringStream = testString.codePoints()
              .mapToObj(c -> String.valueOf((char) c));                // Coversion of string to Character stream


        Ex:

        //The groupingBy() method of Collectors class in Java are used for grouping objects by some property and storing results in a Map instance. In order to use it, we always need to specify a property by which the grouping would be performed. This method provides similar functionality to SQL’s GROUP BY clause. Return value: It returns a collector as a map.

        // Function.identity() retuns a 'Function' with one method that does nothing more than returning the argument it gets, ex: e->e

            Character result =  input.chars()      //string stream, ie IntStream intStream = input.chars();
             .mapToObj(i -> Character.toLowerCase(Character.valueOf((char) i))) //convert to lowercase & then to Character object
             .collect(Collectors.groupingBy(Function.identity(), LinkedHashMap::new, Collectors.counting())) //store in a map with the count
             .entrySet().stream()
             .filter(entry -> entry.getValue() == 1L)
             .map(entry -> entry.getKey())
             .findFirst().get();
        
            return result; // returns first non repeatable character of string

        Anothor ex groupingBy:

            // Get the List
            List<String> g
                = Arrays.asList("geeks", "for", "geeks");
     
            // Collect the list as map
            // by groupingBy() method
            Map<String, Long> result
                = g.stream().collect(
                    Collectors.groupingBy(
                        Function.identity(),
                        Collectors.counting()));


Q)  program to sort hashmap by value??

Ans)

    Ex:

        // function to sort hashmap by values
        public static HashMap<String, Integer> sortByValue(HashMap<String, Integer> hm)
        {
            // Create a list from elements of HashMap
            List<Map.Entry<String, Integer> > list =
                   new LinkedList<Map.Entry<String, Integer> >(hm.entrySet());
     
            // Sort the list
            Collections.sort(list, new Comparator<Map.Entry<String, Integer> >() {
                public int compare(Map.Entry<String, Integer> o1,
                                   Map.Entry<String, Integer> o2)
                {
                    return (o1.getValue()).compareTo(o2.getValue());
                }
            });
             
            // put data from sorted list to hashmap
            HashMap<String, Integer> temp = new LinkedHashMap<String, Integer>();
            for (Map.Entry<String, Integer> aa : list) {
                temp.put(aa.getKey(), aa.getValue());
            }
            return temp;
        }

Q)  explain architecture of JVM??

Ans) Java Architecture is a collection of components, i.e., JVM, JRE, and JDK
    
    JVM(Java Virtual Machine) acts as a run-time engine to run Java applications. JVM is the one that actually calls the main method present in a java code. JVM is a part of JRE(Java Runtime Environment).

    When we compile a .java file, .class files(contains byte-code) with the same class names present in .java file are generated by the Java compiler. This .class file goes into various steps when we run it. These steps together describe the whole JVM. 

    1) Class loader:
        It is mainly responsible for three activities. 

        i) The Class loader reads the “.class” file, generate the corresponding binary data and save it in the method area. For each “.class” file, JVM stores the following information in the method area. 

            The fully qualified name of the loaded class and its immediate parent class.
            Whether the “.class” file is related to Class or Interface or Enum.
            Modifier, Variables and Method information etc

            After loading the “.class” file, JVM creates an object of type Class to represent this file in the heap memory. Please note that this object is of type Class predefined in java.lang package. These Class object can be used by the programmer for getting class level information like the name of the class, parent name, methods and variable information etc. To get this object reference we can use getClass() method of Object class.

            Note: For every loaded “.class” file, only one object of the class is created. 

        ii) Verification: It ensures the correctness of the .class file i.e. it checks whether this file is properly formatted and generated by a valid compiler or not.

        iii) Initialization: In this phase, all static variables are assigned with their values defined in the code and static block(if any). This is executed from top to bottom in a class and from parent to child in the class hierarchy. 

            In general, there are three class loaders : 

            Bootstrap class loader: Every JVM implementation must have a bootstrap class loader, capable of loading trusted classes. It loads core java API classes present in the “JAVA_HOME/jre/lib” directory. This path is popularly known as the bootstrap path. It is implemented in native languages like C, C++.

            Extension class loader: It is a child of the bootstrap class loader. It loads the classes present in the extensions directories “JAVA_HOME/jre/lib/ext”(Extension path) or any other directory specified by the java.ext.dirs system property.

            System/Application class loader: It is a child of the extension class loader. It is responsible to load classes from the application classpath. It internally uses Environment Variable which mapped to java.class.path.


    JVM Memory:

        Heap, stack ,method area etc 

    Execution Engine:

        Execution engine executes the “.class” (bytecode). It reads the byte-code line by line, uses data and information present in various memory area and executes instructions. Garbage Collector is part of this engine.


Q)  what changes you will do in a class to use it as key in hashmap??

Ans)   use immutable objects as keys in a HashMap Or we will need to override hashcode and equals method.
    
    hashCode() -HashMap provides put(key, value) for storing and get(key) method for retrieving Values from HashMap. When put() method is used to store (Key, Value) pair, HashMap implementation calls hashcode on Key object to calculate a hash that is used to find a bucket where Entry object will be stored. When get() method is used to retrieve value, again key object is used to calculate a hash which is used then to find a bucket where that particular key is stored.

    equals() - equals() method is used to compare objects for equality. In case of HashMap key object is used for comparison, also using equals() method Map knows how to handle hashing collision (hashing collision means more than one key having the same hash value, thus assigned to the same bucket. In that case objects are stored in a linked list, refer figure for more clarity. Where hashCode method helps in finding the bucket where that key is stored, equals method helps in finding the right key as there may be more than one key-value pair stored in a single bucket.

    Problem:

        class Person
        {
            private String name;
            private int age;
         
            // Constructor
            Person(String name, int age)
            {
                this.name = name;
                this.age = age;
            }
         
            @Override
            public String toString() {
                return "{" + name + ", " + age + "}";
            }
        }


        Person p1 = new Person("John", 20);
        Person p2 = new Person("John", 20);
        Person p3 = new Person("Carol", 16);
 
        Set<Person> set = new HashSet<>();
        set.add(p1);
        set.add(p2);
        set.add(p3);
 
        System.out.println(set);   // [{John, 20}, {John, 20}, {Carol, 16}]

        In the above example, p1 and p2 are considered two separate keys by the HashSet even though they have equal fields. Therefore, two keys will only be equal if they are, in fact, the same object.

    Solution:

        If we need key equality on different objects with the same value of instance variables, we need to implement equals() and hashCode() on the key.

        class Person
        {
            private String name;
            private int age;
         
            // Constructor
            Person(String name, int age)
            {
                this.name = name;
                this.age = age;
            }
         
            @Override
            public boolean equals(Object ob)
            {
                if (ob == this) {
                    return true;
                }
         
                if (ob == null || ob.getClass() != getClass()) {
                    return false;
                }
         
                Person p = (Person) ob;
                return Objects.equals(name, p.name) && p.age == age;
            }
         
            @Override
            public int hashCode() {
                return Objects.hash(name, age);
            }
         
            @Override
            public String toString() {
                return "{" + name + ", " + age + "}";
            }
        }

        Person p1 = new Person("John", 20);
        Person p2 = new Person("John", 20);
        Person p3 = new Person("Carol", 16);
 
        Set<Person> set = new HashSet<>();
        set.add(p1);
        set.add(p2);
        set.add(p3);
 
        System.out.println(set); // [{Carol, 16}, {John, 20}]

Q) Bulder pattern example ?

Ans)

    public static final class EmployeeBuilder {
        private long id;
        private String name;
        private Date dateOfBirth;
        private BigDecimal salary;

        private EmployeeBuilder() {
        }

        public static EmployeeBuilder anEmployee() {
            return new EmployeeBuilder();
        }

        public static EmployeeBuilder anEmployee(Employee employee) {
            return anEmployee().withId(employee.getId()).withName(employee.getName()).withDateOfBirth(employee.getDateOfBirth()).withSalary(employee.getSalary());
        }

        public EmployeeBuilder withId(long id) {
            this.id = id;
            return this;
        }

        public EmployeeBuilder withName(String name) {
            this.name = name;
            return this;
        }

        public EmployeeBuilder withDateOfBirth(Date dateOfBirth) {
            this.dateOfBirth = dateOfBirth;
            return this;
        }

        public EmployeeBuilder withSalary(BigDecimal salary) {
            this.salary = salary;
            return this;
        }

        public Employee build() {
            return new Employee(this);
        }
    }



Q)  How did you implement logging in your microservices application??

Ans)
    
    We have application level logging used slf4j and distributed logging/ tracing.

    What is Distributed Tracing?

    We must have a unique request/transaction reference which should be passed across all the calls, to all the dependent microservices, in the call chain. This unique reference is usually referred as Correlation-id or Trace Id.

    The Trace Id is created when the request hits the very first service in the chain. For subsequent calls, already existing Trace Id is passed along, typically as an http header attribute. This pattern is called — Distributed Tracing.

    How to implement tracing ??

    Implementing Tracing — In this part we will focus on pattern implementation to the core including Trace Id generation, passing it along the service calls and including it in the logs. We will be using Spring Cloud Sleuth library to achieve this.

    Enabling Tracing System — In this part we will focus on how the traces are collected and visualized to get better insights into the service interactions. This reduces time in triage by contextualizing errors and delays. We will be using Zipkin to implement this.

    Example we have 2 services Product Catalog Service and Shopping Cart Service.

    Step 1: Lets add a dependency of spring-cloud-starter-sleuth to the Product Catalog Service’s pom.xml

    Step 2: Update getProductDetails method of ProductCatalogService.java and add some sample log statements. We are using slf4j to get the Logger instance.

        @RestController
        public class ProductCatalogService {
        @Autowired
            private MongoTemplate mongoTemplate;
            private Logger logger = LoggerFactory.getLogger(ProductCatalogService.class);

        @GetMapping("/product/{id}")
            public Product getProductDetails(@PathVariable String id) {
                logger.info("get product details - process started");
                Product product = mongoTemplate.findById(id, Product.class);
                if (product != null)
                    logger.info("get product details - product found");
                else
                    logger.info("get product details - product not found");
        return product;
            }
        }

    Step 3: Update src/main/resources/application.properties to use the log level as TRACE and to include the service name as product_catalog.

        spring.application.name=product_catalog
        logging.level.org.springframework.web.servlet.DispatcherServlet=TRACE

    Thats all we need, to add Trace Id in our requests and logs. 

    Now when we hit "product/{id}" then we can see logs in console like [product_catalog, 86d9....f68, 54ft91f...m9892]. If you see the tracing log part, log statement is carrying Service Name, Trace Id & Span Id in the respective order. 

    Lets make the similar changes in the Shopping Cart Service.

    If same request calls both services then trace ID will be same.With this id we can track all the relevant logs from both the services. We can troubleshoot any issues navigating the logs based on Trace Id.

    Zipkin receives the traces from all the different services. It aggregates them based on the Trace Id and provide multiple views for lookup. 

    We can use Zipkin view for To identify requests status, To identify service latency.

    We need to download the zipkin archive, from its website, and run it through the Java command. 

    java -jar zipkin.jar

    This will start the zipkin server at http://localhost:9411. Now we need to ensure our services (Shopping Cart & Product Catalog) can send the traces to it. To enable this, we need to add a new dependency — spring-cloud-sleuth-zipkin in our services. Update pom.xml with the following change

    <dependency>
     <groupId>org.springframework.cloud</groupId>
     <artifactId>spring-cloud-sleuth-zipkin</artifactId>
    </dependency>

    With this library on classpath, when the span is closed against a request, the trace information is sent to Zipkin over HTTP. By default it connects to the local address — http://localhost:9411/. You can update its location in application.properties as below

    spring.zipkin.baseUrl=http://sample-hostname:sample-port/

    This request is visible with the specific Trace Id, Start Time and Total Processing Time of request. If the request is failed, you will know here itself. The entry includes all the services getting called in the request processing.

    This displays the response status and the latency figures for each of the services. If you want to track a particular request, just type in the trace id, and you will know the exact status, failure point.


Q)  circuit breaker and internal working??

Ans)

    You wrap a protected function call in a circuit breaker object, which monitors for failures. Once the failures reach a certain threshold, the circuit breaker trips, and all further calls to the circuit breaker return with an error or with some alternative service or default message, without the protected call being made at all. 

    Ex:
        @HystrixCommand(fallbackMethod = "reliable")
         public String readingList() {
          URI uri = URI.create("http://localhost:8090/recommended");
          return this.restTemplate.getForObject(uri, String.class);
         }

         public String reliable() {
          return "Cloud Native Java (O'Reilly)";
         }

    Example with Feign client:

        @Configuration
        @EnableFeignClients
        @EnableCircuitBreaker
        public class FeignConfiguration {

        }

        @FeignClient(name = "videos", url = "http://localhost:9090/videos", fallback = VideoClientFallback.class)
        public interface VideoClient {

            @PostMapping(value = "/api/videos/suggest")
            List<Suggestion> suggest(@RequestBody ViewingHistory history);

        }

        Component
        public class VideoClientFallback implements VideoClient {

            @Override
            public List<Suggestion> suggest(ViewingHistory history) {
              // Degraded service: no suggestion to offer
              return new ArrayList<>();
            }

        }

        A configuration property has to be added to the application.properties file of the Spring Boot application to tell Feign to enable Hystrix.

        feign.hystrix.enabled: true

        Whenever the remote service is unavailable, the suggest method of the VideoClientFallback will be called.


Q)  write a code to consume a api??

Ans)  we can use RestTemplate,  FeignClient etc

Q)  using streams find second highest salary of employee?? Find oldest employee in organization?? using java 8


Ans)
        1)

            List<Employee> sortedList = employeeList.stream()
            .sorted(Comparator.comparingDouble(Employee::getSalary).reversed()).collect(Collectors.toList());    

            sortedList.get(1); // as second highest salary

        2) 
            
            List<Employee> sortedList = employeeList.stream()
            .sorted(Comparator.comparingDouble(Employee::getAge).reversed()).collect(Collectors.toList());    

            sortedList.get(0); // as oldest employee


Q) Java 8 collectors class,  Option class,  forEach method, parallel array sorting,  nashorm javasrcript engine ?? 

Ans)
    -> The toList collector can be used for collecting all Stream elements into a List instance. The important thing to remember is that we can't assume any particular List implementation with this method. If we want to have more control over this, we can use toCollection instead.

        Ex:

        List<String> result = givenList.stream()
          .collect(toList());

    -> The toSet collector can be used for collecting all Stream elements into a Set instance. The important thing to remember is that we can't assume any particular Set implementation with this method. If we want to have more control over this, we can use toCollection instead

        Ex:

            Set<String> result = givenList.stream()
                .collect(toSet());

    -> To collection,

        Ex:
            ArrayList<String> newList = srcCollection.stream().collect(toCollection(ArrayList::new));

            HashSet <String> newSet = srcCollection.stream().collect(toCollection(HashSet::new));

    -> The toMap collector can be used to collect Stream elements into a Map instance. To do this, we need to provide two functions:

        keyMapper
        valueMapper
        We'll use keyMapper to extract a Map key from a Stream element, and valueMapper to extract a value associated with a given key.

        Let's collect those elements into a Map that stores strings as keys and their lengths as values:

        Ex:
            Map<String, Integer> result = givenList.stream()
              .collect(toMap(Function.identity(), String::length))

        Function.identity() is just a shortcut for defining a function that accepts and returns the same value.

    -> CollectingAndThen is a special collector that allows us to perform another action on a result straight after collecting ends
       Let's collect Stream elements to a List instance, and then convert the result into an ImmutableList instance:

        Ex:

            List<String> result = givenList.stream()
              .collect(collectingAndThen(toList(), ImmutableList::copyOf))

    -> Joining collector can be used for joining Stream<String> elements.

        Ex: 

            String result = givenList.stream()
              .collect(joining());        // "abbcccdd"

        Ex:

            String result = givenList.stream()
              .collect(joining(" "));            // "a bb ccc dd"

        Ex:

            String result = givenList.stream()
              .collect(joining(" ", "PRE-", "-POST")); // "PRE-a bb ccc dd-POST"

    -> Counting is a simple collector that allows for the counting of all Stream elements.

        Ex:

            Long result = givenList.stream()
              .collect(counting());

    -> SummarizingDouble/Long/Int is a collector that returns a special class containing statistical information about numerical data in a Stream of extracted elements.

        We can obtain information about string lengths by doing:

        Ex:

        DoubleSummaryStatistics result = givenList.stream()
          .collect(summarizingDouble(String::length));
        In this case, the following will be true:

        assertThat(result.getAverage()).isEqualTo(2);
        assertThat(result.getCount()).isEqualTo(4);
        assertThat(result.getMax()).isEqualTo(3);
        assertThat(result.getMin()).isEqualTo(1);
        assertThat(result.getSum()).isEqualTo(8);

    -> Collectors.averagingDouble/Long/Int()
        AveragingDouble/Long/Int is a collector that simply returns an average of extracted elements.

        We can get the average string length by doing:

        Double result = givenList.stream()
          .collect(averagingDouble(String::length));

        // Similiary, SummingDouble/Long/Int is a collector that simply returns a sum of extracted elements.
        // MaxBy/MinBy collectors return the biggest/smallest element of a Stream according to a provided Comparator instance.

        Ex:
            Optional<String> result = givenList.stream()
              .collect(maxBy(Comparator.naturalOrder()));

    -> GroupingBy collector is used for grouping objects by some property, and then storing the results in a Map instance.

        We can group them by string length, and store the grouping results in Set instances:

        Map<Integer, Set<String>> result = givenList.stream()
          .collect(groupingBy(String::length, toSet()));
        This will result in the following being true:

        assertThat(result)
          .containsEntry(1, newHashSet("a"))
          .containsEntry(2, newHashSet("bb", "dd"))
          .containsEntry(3, newHashSet("ccc"));
        We can see that the second argument of the groupingBy method is a Collector. In addition, we're free to use any Collector of our choice.

    -> PartitioningBy is a specialized case of groupingBy, then collects Stream elements into a Map instance that stores Boolean values as keys and collections as values. Under the “true” key, we can find a collection of elements matching the given Predicate, and under the “false” key, we can find a collection of elements not matching the given Predicate.

        Ex:
            Map<Boolean, List<String>> result = givenList.stream()
                .collect(partitioningBy(s -> s.length() > 2))
            
            This results in a Map containing:

            {false=["a", "bb", "dd"], true=["ccc"]}


    Parallel array sorting:

    Java 8 introduced a new method called as parallelSort() in java.util.Arrays Class. It uses Parallel Sorting of array elements.

    1)    The array is divided into sub-arrays and that 
       sub-arrays is again divided into their sub-arrays, 
       until the minimum level of detail in a set of array.
    2. Arrays are sorted individually by multiple thread. 
    3. The parallel sort uses Fork/Join Concept for sorting.
    4. Sorted sub-arrays are then merged.

    Ex:

        // Creating an array
        int numbers[] = { 9, 8, 7, 6, 3, 1 };
  
        // Printing unsorted Array
        System.out.print("Unsorted Array: ");
        // Iterating the Elements using stream
        Arrays.stream(numbers)
            .forEach(n -> System.out.print(n + " "));
        System.out.println();
  
        // Using Arrays.parallelSort()
        Arrays.parallelSort(numbers);
  
        // Printing sorted Array
        System.out.print("Sorted Array: ");
        // Iterating the Elements using stream
        Arrays.stream(numbers)
            .forEach(n -> System.out.print(n + " "));

    // For reverse sort array use , Arrays.sort(array, Collections.reverseOrder());
    // For reverse sort list use , Collections.sort(list, Collections.reverseOrder());


Q) How do you debug streams ??

Ans)

    The peek() method of the Stream class can be very useful to debug and understand streams in Java 8. You can use the peek() method to see the elements as they flow from one step to another like when you use the filter() method for filtering, you can actually see how filtering is working like lazy evaluation as well as which elements are filtered.

    Consider the following example, which calls the peek() method after each step in a Stream pipeline involving filter() and map()  methods:

    List<String> result = Stream.of("EURO/INR", "USD/AUD", "USD/GBP", "USD/EURO")
        .filter(e -> e.length() > 7)
        .peek(e -> System.out.println("Filtered value: " + e))
        .map(String::toLowerCase)
        .peek(e -> System.out.println("Mapped value: " + e))
        .collect(Collectors.toList());

    Many of you will think that after the first filter() execution you will get a Stream containing two elements "EURO/INR" and "USD/EURO" and peek() will print those two elements.

    Well, that's not the case, since Streams are executed lazily, nothing will happen until the collect() method will execute, which is the terminal method.

    Output:
        Filtered value: EURO/INR
         Mapped value: euro/inr 
         Filtered value: USD/EURO 
         Mapped value: usd/euro


    The key point to note here is that values are filtered and mapped one by one, not together. It means the code is executed backward when the collect() method calls the Collectors.toList() to get the result in a List, it asks map() function which in turn asks the filter() method.

    Since filter() is lazy it returns the first element whose length is greater than 7 and sits back until map() asks again.

Q) Example reduce() method in java 8?

Ans)   
    
    Many times, we need to perform operations where a stream reduces to single resultant value, for example, maximum, minimum, sum, product, etc. Reducing is the repeated process of combining all elements.

    Ex:

        // creating a list of Strings
        List<String> words = Arrays.asList("GFG", "Geeks", "for",
                                           "GeeksQuiz", "GeeksforGeeks");
  
        // The lambda expression passed to
        // reduce() method takes two Strings
        // and returns the longer String.
        // The result of the reduce() method is
        // an Optional because the list on which
        // reduce() is called may be empty.
        Optional<String> longestString = words.stream()
                                   .reduce((word1, word2)
                             -> word1.length() > word2.length()
                                           ? word1 : word2);
  
        // Displaying the longest String
        longestString.ifPresent(System.out::println);

    Output: GeeksforGeeks

    Ex:

        // String array
        String[] array = { "Geeks", "for", "Geeks" };
  
        // The result of the reduce() method is
        // an Optional because the list on which
        // reduce() is called may be empty.
        Optional<String> String_combine = Arrays.stream(array)
                                           .reduce((str1, str2)
                                           -> str1 + "-" + str2);
  
        // Displaying the combined String
        if (String_combine.isPresent()) {
            System.out.println(String_combine.get());
        }

        Output: Geeks-for-Geeks

    Ex:

        // Creating list of integers
        List<Integer> array = Arrays.asList(-2, 0, 4, 6, 8);
  
        // Finding sum of all elements
        int sum = array.stream().reduce(0,
                (element1, element2) -> element1 + element2);       // 0 is initial value
  
        // Displaying sum of all elements
        System.out.println("The sum of all elements is " + sum);

        Output: The sum of all elements is 16

    Ex:

        // To get the product of all elements
        // in given range excluding the
        // rightmost element
        int product = IntStream.range(2, 8)
                     .reduce((num1, num2) -> num1 * num2)
                     .orElse(-1);
  
        // Displaying the product
        System.out.println("The product is : " + product);

        Output: The product is : 5040


Q)  Why String is immutable??

Ans)
    
    A String is used as an argument for class loading. Let’s imagine what will happen if String is mutable. In that case, the value of the object can be changed and wrong class can be loaded.

    Immutability provides security so that the correct class is getting loaded by the Classloader.

    String is mostly used as the Object to HashMap keys. Since String is immutable, its hashcode is cached at the time of creation and doesn’t need to be calculated again. This makes it a great candidate for the key in a Map, and it’s processing is fast than other HashMap key objects.

Q) If String is immutable, why is the following statement allowed?

String a = “Generic Class”;

a = “www.genericclass.com”;

Ans) In the above statements, you are not changing the value of the String object. Here, you are basically changing the reference. Previously, your variable a was pointing to an object that had the value “Generic Class."

But, in the second statement, a new object is created with value “www.genericclass.com” and your variable started to point to this new object.

Q)  Difference between Functions and procedures in mySql??

Ans)

The function must return a value but in Stored Procedure it is optional. Even a procedure can return zero or n values. Functions can have only input parameters for it whereas Procedures can have input or output parameters. Functions can be called from Procedure whereas Procedures cannot be called from a Function.    

Q)  Can we overload static methods??

A static method can be overloaded, but can not be overridden in Java. If you declare,  another static method with same signature in derived class than the static method of superclass will be hidden, and any call to that static method in subclass will go to static method declared in that class itself. This is known as method hiding in Java.

Static methods can not be overridden in Java, they can only be hidden.

Now, As per rules of method overriding, if a method is overridden than a call is resolved by the type of object during runtime. This means, in our test class StaticOverrideTest, p.name() in the second the example should call Child class' name() method because the reference variable of type Parent is referring an object of Child, but instead, it call name() method of Parent class itself


    Ex:

        class Parent{ 

            /* * original static method in super class which will be hidden * in subclass. */

             public static void name(){ 

                System.out.println("static method from Parent"); 

             } 

        } 


        class Child extends Parent{ 

            /* * Static method with same signature as in super class, * Since static method can not be overridden, this is called * method hiding. Now, if you call Child.name(), this method * will be called, also any call to name() in this particular * class will go to this method, because super class method is hidden. */ 

            public static void name(){ System.out.println("static method from Child"); } 

        }

        Parent p = new Parent(); 
        p.name(); // should call static method from super class (Parent) 
                // because type of reference variable 
                // p is Parent
         p = new Child(); 
         p.name(); // as per overriding rules this should call to child's static 
                    // overridden method. Since static method can not be overridden 
                    // , it will call parent static method   
                    // because Type of p is Parent. 
        Child c = new Child(); 
        c.name(); // will call child static method because static method 
                    // get called by type of Class


        Output: static method from Parent 
                static method from Parent 
                static method from Child


        Overload allowed example:

            public static void foo() {
                System.out.println("Test.foo() called ");
            }
            public static void foo(int a) {
                System.out.println("Test.foo(int) called ");
            }
            public static void main(String args[])
            {
                Test.foo();
                Test.foo(10);
            }

Q)  How to execute Java class using cmd

Ans)Goto path of class 

    Javac MyClass.java // Compile

    Java Myclass   // Returns result

Q)  write hashMap, then iterate and write output to console??

Ans)
    Ex 1:
    Map<String,String> gfg = new HashMap<String,String>();
     
        // enter name/url pair
        gfg.put("GFG", "geeksforgeeks.org");
        gfg.put("Practice", "practice.geeksforgeeks.org");
        gfg.put("Code", "code.geeksforgeeks.org");
        gfg.put("Quiz", "quiz.geeksforgeeks.org");
         
        // using for-each loop for iteration over Map.entrySet()
        for (Map.Entry<String,String> entry : gfg.entrySet())
            System.out.println("Key = " + entry.getKey() +
                             ", Value = " + entry.getValue());

    Ex 2:
         
        // using keySet() for iteration over keys
        for (String name : gfg.keySet())
            System.out.println("key: " + name);

        // using values() for iteration over values
        for (String url : gfg.values())
            System.out.println("value: " + url);

    Ex 3:

        // using iterators
        Iterator<Map.Entry<String, String>> itr = gfg.entrySet().iterator();
         
        while(itr.hasNext())
        {
             Map.Entry<String, String> entry = itr.next();
             System.out.println("Key = " + entry.getKey() +
                                 ", Value = " + entry.getValue());
        }

    Ex 4:

        // forEach(action) method to iterate map
        gfg.forEach((k,v) -> System.out.println("Key = "
                + k + ", Value = " + v));


Q)  what is runtime polymorphism and compile time polymorphism ??  Write a program for both??

Ans) polymorphism means having many forms.
    
        In Compile time Polymorphism, the call is resolved by the compiler. We can acheive this by method overloading. Method overloading is the compile-time polymorphism where more than one methods share the same name with different parameters or signature and different return type.    

            Ex:

                // First addition function
                public static int add(int a, int b)
                {
                    return a + b;
                }
             
                // Second addition function
                public static double add(
                    double a, double b)
                {
                    return a + b;
                }
             
                // Driver code
                public static void main(String args[])
                {
                    // Here, the first addition
                    // function is called
                    System.out.println(add(2, 3));
             
                    // Here, the second addition
                    // function is called
                    System.out.println(add(2.0, 3.0));
                }

        In Run time Polymorphism, the call is not resolved by the compiler. We can achieve this by method overriding. Method overriding is the runtime polymorphism having same method with same parameters or signature, but associated in different classes.


Q)  program to find vowels and duplicates in string using streams??

Ans)  String input = "hello hello";

Long count = input.chars().filter((x) ->{
return (x == 'a'  || x == 'e' ||  x == 'i'  ||  x == 'o'|| x == 'u');
}).count();

Map<String,Long> out = input. chars. mapToObj((x) -> (char)  x).collect(Collectors.groupingBy(Object::toString, Collectors. counting()));

//or we can use Collectors.groupingBy(Function.identity(), Collectors.counting());

out.forEach((key, value)  -> System. out. println(key,value));

Q)  what are terminals and intermediate operations?? 

Ans)
    map() / mapToObject / Filter etc are intermidiate operations and Collect(), Count() etc are terminal operations 

Q)  print numbers 1 to 10 sequentially using 3 threads ??

Ans)
        
        public class PrintSequenceRunnable implements Runnable{
 
                public int PRINT_NUMBERS_UPTO=10;
                static int  number=1;
                int remainder;
                static Object lock=new Object();
             
                PrintSequenceRunnable(int remainder)
                {
                    this.remainder=remainder;
                }
             
                @Override
                public void run() {
                    while (number < PRINT_NUMBERS_UPTO-1) {
                        synchronized (lock) {
                           while (number % 3 != remainder) {
                                try {
                                    lock.wait();
                                } catch (InterruptedException e) {
                                    e.printStackTrace();
                                }
                            }
                          System.out.println(Thread.currentThread().getName() + " " + number);
                          number++;
                          lock.notifyAll();
                        }
                    }
                }
            }


        PrintSequenceRunnable runnable1=new PrintSequenceRunnable(1); 
        PrintSequenceRunnable runnable2=new PrintSequenceRunnable(2);
        PrintSequenceRunnable runnable3=new PrintSequenceRunnable(0);
 
        Thread t1=new Thread(runnable1,"T1");
        Thread t2=new Thread(runnable2,"T2");
        Thread t3=new Thread(runnable3,"T3");
 
        t1.start();
        t2.start();
        t3.start();   



Q)  what atomicinteger in java??

Ans)    
    An AtomicInteger is used in applications such as atomically incremented counters, and cannot be used as a replacement for an Integer .

    The primary use of AtomicInteger is when you are in a multithreaded context and you need to perform thread safe operations on an integer without using synchronized .

        Ex:

            class ProcessingThread implements Runnable {
                private AtomicInteger count = new AtomicInteger();

                @Override
                public void run() {
                    for (int i = 1; i < 5; i++) {
                        processSomething(i);
                        count.incrementAndGet();
                    }
                }

                public int getCount() {
                    return this.count.get();
                }

                private void processSomething(int i) {
                    // processing some job
                    try {
                        Thread.sleep(i * 1000);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }

            }

Q)  can you make constructor final??

Ans) Java won't allow it because it is of no use as you cannot override constructor in child class.

Q)  what will happen if you call return statement or System.exit in try or catch block , will finally block executed??

Ans)   In case of return statement, finally block will be executed but not in case of System.exit.

Q) why Thread.stop() depreciated??

Ans)
    
    In Java there's no clean, quick or reliable way to stop a thread. Instead, Threads rely on a cooperative mechanism called Interruption. This means that Threads could only signal other threads to stop, not force them to stop.


Q)  why we should use ThreadPoolExecutor , when we have Executor framework??

Ans) ExecutorService executor = Executors.newFixedThreadPool(5);  //Example of using executor framework, 5 is thread size


//Understanding parameters, Take this example. Starting thread pool size is 1, core pool size is 5, max pool size is 10 and the queue is 100.As requests come in threads will be created up to 5, then tasks will be added to the queue until it reaches 100. When the queue is full new threads will be created up to maxPoolSize. Once all the threads are in use and the queue is full tasks will be rejected.

int  corePoolSize  =    5;
int  maxPoolSize   =   10;
long keepAliveTime = 5000;

int queue = 100

ExecutorService threadPoolExecutor =
        new ThreadPoolExecutor(
                corePoolSize,
                maxPoolSize,
                keepAliveTime,
                TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<Runnable>(queue)
                );                                    // Example using threadpool

There are scenarios where default configuration is not suitable.  Like instead of LinkedBlockingQueue a PriorityQueue is needed.  While using ThreadPoolExecutor we can pass any other configuration as parameter.

Although ExecutorService internally uses ThreadPoolExecutor class.


Q)  what is Boxed streams??

Ans)  To convert stream of primitives we must first box the elements in their wrapper class and then collect them.  This type of stream is called boxed stream.

Ex:

List<Integer> l = IntStream. of(1.2.3.4.5).boxed().collect(Collectors::toList());

Q)  List spring core and stereotype annotations??

Ans)  Spring core:

@Qualifier,  @Autowired,  @Configuration,  @ComponentScan,  @Required,  @Bean,  @Lazy,  @Value

Spring stereotype : example spring mvc specific

@Component, @Controller, @Service, @Repository

Q)  Covariant return type??

Ans)  Change return type of overriden method called Covariant return type :

class SuperClass {
   SuperClass get() {
      System.out.println("SuperClass");
      return this;
   }
}
public class Tester extends SuperClass {
   Tester get() {
      System.out.println("SubClass");
      return this;
   }
   public static void main(String[] args) {
      SuperClass tester = new Tester();
      tester.get();
   }
}


Q)  What performance benefits of using streams??

Ans)

    1) Parallelism:

    Parallelism utilizes hardware capabilities at their best, as nowadays, more CPU cores are available on a computer, so it doesn't make sense to have a single thread in a multi-core system. Designing and writing multi-threaded applications is challenging and error-prone, hence Streams has two implementations: sequential and parallel. Using parallel Streams is easy and no expertise is needed for thread handling.

    In Java Streams, parallelism is achieved by using the  Fork-Join principle. As per the Fork-Join principle, it divides larger tasks into smaller sub-tasks (known as forking), and then processes the sub-tasks in parallel to utilize all the available hardware, then combines the results together (known as Join) to form an integrated result.

    Ex:

        System.out.println(myList.parallelStream().filter(value -> value % 2 == 0).mapToInt(Integer::intValue).sum());

    2) Laziness:

    As we know, Java8 Streams have two types of operations, known as Intermediate and Terminal. These two operations are meant for processing and providing the end results, respectively. You might have seen that if a terminal operation is not associated with intermediate operations, it can't be executed.

    Consider a situation where we have a snippet of streams, but with intermediate operations only, and the terminal operations are placed later in the application (which may or may not be required and depends upon the user request). In this case, the streams intermediate operations will create another stream for terminal operations but will not perform the actual processing; which would be helpful to improve performance.

    3) Short-Circuit Behavior:

    This is another way of optimizing the Streams processing. Short-circuiting will terminate the processing once condition met. There are a number of short-circuiting operations available. For e.g. anyMatch, allMatch, findFirst, findAny, limit, etc.

    This is another way of optimizing the Streams processing. Short-circuiting will terminate the processing once condition met. There are a number of short-circuiting operations available. For e.g. anyMatch, allMatch, findFirst, findAny, limit, etc.

    Ex:

        employeeNameStreams = employees.stream().filter(e -> e.getId() % 2 == 0)
           .map(employee -> {
            System.out.println("In Map - " + employee.getName());
            return employee.getName();
           }).limit(100).collect(Collectors.toList());


    If you run the above code, you will see the huge performance boost, as it took just 6 milliseconds on my machine. Here, the limit()  method will terminate the condition once met.


Q)  Any change in memory management in java 8 ??

Ans)  Removed PermGen and introducted Metaspace which increases the size dynamically.

Q) What are different servers you have worked with ?? Difference between tomcat, JBoss and Glassfish ??

Ans) Both JBoss and Tomcat are Java servlet application servers, but JBoss is a whole lot more. The substantial difference between the two is that JBoss provides a full Java Enterprise Edition (Java EE) stack, including Enterprise JavaBeans and many other technologies that are useful for developers working on enterprise Java applications.

Java EE is a specification which is implemented by various softwares such as JBoss and Glassfish.

Tomcat is much more limited. One way to think of it is that JBoss is a Java EE stack that includes a servlet container and web server, whereas Tomcat, for the most part, is a servlet container and web server.


Q) Java EE vs Spring ??

Ans) 

To develop enterprise application,open sourced framework is called spring.

J2ee can independently develop web applications without spring.

But still spring frameworks are developed and using the j2ee


Q) JAR vs WAR vs EAR ??


Ans) 

A JAR file has all the components required to make a self-contained executable Java application. It consists of Java source codes, XML based configuration data, manifest file, JSON based data files, etc. In other words, a JAR file is a combination of all these files as a single compressed file. The compressed file reduces the size of the application. It is also easier to send that file through the network to various systems or platforms.

To develop Java-based applications, the programmer should install Java Development Kit (JDK) into the machine. Each JDK has JAR utility to support JAR files. It allows creating new JAR files with a manifest file and extracting all the content of a JAR file onto the file system. 


A WAR file contains the files of a web project. It can have servlet, JSP, XML, HTML, CSS and JavaScript files. These files can be deployed on servlet/JSP container. The WAR files are inside the WEB-INF folder of the project. As a WAR file combines all the files into a single unit, it takes less time to exchange a file from the client to server.

It is possible to deploy a WAR file by using the server control panel. Another method is by manually locating a WAR file in a specific folder of the server. In order to deploy a WAR file in a server such as Tomcat manually, the programmer can go to the webapps directory of Tomcat and paste the WAR file in that directory.  Otherwise, the server extracts the WAR file internally, when executing the web project.


An ear file is like a war file, but designed to be used by a Java EE application server. “EE” stands for Enterprise Edition. Java EE is everything in the standard Java plus a bunch of extra APIs (last time I counted, 24 more APIs) that are useful for building enterprise applications. Enterprise applications means applications that meet the needs particular to large companies (aka enterprises), which usually means things like connecting to a lot of systems, being used by a lot of people, and being high availability and high reliability.

The main requirement for an ear file is the /META-INF/application.xml, which contains Java EE deployment descriptors, configuration files for how the Java EE server will assemble and run the enterprise application contained in the ear file. The ear file can, like war and jar files, contain other jars, and it can also contain a war file (which will need an entry in the application.xml for the enterprise server to deploy it as an application).


Q)  If we don't want to use any embedded server like tomcat,  jetty etc,  how can we configure external configuration ?? How to package project in spring ??

Ans) 

Spring Boot has the ability to set up your applications with the built-in embedded tomcat.

Most of the time it is expected that the application can run on the external Tomcat server or JBoss server. And the reason behind is the operational infrastructure decided by the organization.

And it is very important to know that the Spring Boot application won’t start by simply deploying it to the external Tomcat or web server. Below are some reasons why this happened:

1) By default, when you run the Spring Boot application then it will create a jar file for you and when it comes to the Tomcat, then it requires a WAR file.
2) And in the Tomcat server environment, we cannot use the embedded server.
3) Tomcat server won’t hit the main method. Hence there is a need to start the application as a normal Spring application.

The above given are the use cases that explain why the Spring Boot application won’t start.

First, we extend our main class to SpringBootServletInitializer. This tells spring that your main class will be the entry point to initialize your project in server.

internally uses ThreadPoolExecutor class

@SpringBootApplication
public class Application extends SpringBootServletInitializer{
 
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}


Next, we overload the configure method of SpringBootServletInitializer. We tell spring to build the sources from our Main class. Your final Main class should look like this:


internally uses ThreadPoolExecutor class

@SpringBootApplication
public class Application extends SpringBootServletInitializer{
 
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
 
    @Override
    protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {
        return builder.sources(Application.class);
    }
}

Finally, we tell maven to package the project in to WAR. In your pom.xml, change the attribute value for packaging from jar to war

<packaging>war</packaging>

Build project using:

mvn package

With this, you can now package your application with jar or war file. Just change the packaging to either jar or war.

Q) What is deployment descriptor ??


Ans) A web application's deployment descriptor describes the classes, resources and configuration of the application and how the web server uses them to serve web requests.  It defines servlets, their mappings, servlet filters. 

Annotations are an alternative to deployment descriptors that were required by older versions of Enterprise applications (J2EE 1.4 and earlier).

    

Q) Program to get latest first 3 rows from table in hibernate ?  // ie limit and order by

Ans)

    Pageable topTwenty = PageRequest.of(0, 20);
    List<SLSNotification> notifications = repository.findByUserIdOrderBySNumber("101", topTwenty);

Q) Program to group by from table in hibernate ?  // ie group by

Ans)
    
    Define this method in interface.

    @Query("select salary from users group_by username")
    public List<CategoryGroup> groupBy();


Q) Program to write aggregation ??

Ans) We can also return query as another object like this ,

    public class AggregateResults {

            private final double rating;
            private final int totalRatings;

            public AggregateResults(double rating, long totalRatings) {
                this.rating = rating;
                this.totalRatings = (int) totalRatings;
            }

            public double getRating() {
                return rating;
            }

            public int getTotalRatings() {
                return totalRatings;
            }
        }


        @Query("SELECT new org.magnum.mobilecloud.video.model.AggregateResults(
            AVG(rating) as rating, 
            COUNT(rating) as TotalRatings) 
            FROM UserVideoRating
            WHERE videoId=:videoId")
        public AggregateResults findAvgRatingByVideoId(@Param("videoId") long videoId);


Q) Program to write Order by ?

Ans)

    public List<StudentEntity> findAllByOrderByIdAsc(); // Returns all order by id

    findByNameAndLocationByOrderByProgDateStartTimeAsc(String name, String location);

Q) Program to query using like clause in hibernate ?  

Ans)
    List<Registration> findByPlaceLike(String place);
    
    List<Registration> findByPlaceStartingWith(String place);
    List<Registration> findByPlaceEndingWith(String place);

    List<Registration> findByPlaceContaining(String place);

Q) Program to write update query ? 

Ans) Define this method in interface.
    
    @Query is for defining custom query and @Modifying is for telling spring-data-jpa that this query is an update operation and it requires executeUpdate() not executeQuery().

    @Modifying
    @Query("update Customer u set u.phone = :phone where u.id = :id")
    void updatePhone(@Param(value = "id") long id, @Param(value = "phone") String phone);

    or 

    public void updateCustomer(CustomerDto dto) {
        Customer myCustomer = repo.findById(dto.id);
        mapper.updateCustomerFromDto(dto, myCustomer);
        repo.save(myCustomer);
    }

Q) Program to write multiple query using and , or ??

Ans) 
    Option 1) using JPA,

        public class Employee {
    
            private String name;
            
            private String location;
            
            //setters and getters
        }

        findByNameAndLocation(String name, String location); // Returns where name and location matches

        findByNameOrLocation(String name, String location) // Returns where name or location matches

    Option 2) Using @Query
    
    public interface UserRepository extends JpaRepository<User, Long> {

       @Query("select u from User u where u.emailAddress = ?")
       User findByEmailAddress(String emailAddress);
    }

Q) How to decode JWT and get payload data ??

Ans)

    First, let's understand the structure of a JWT:

    header
    payload (often referred to as body)
    signature
    The signature is optional. A valid JWT can consist of just the header and payload sections. However, we use the signature section to verify the contents of the header and payload for security authorization.

    Sections are represented as base64 encoded strings separated by a period (‘.') delimiter. By design, anyone can decode a JWT and read the contents of the header and payload sections. But we need access to the secret key used to create the signature to verify a token's integrity.

    We can decode a token using built-in Java functions.

    First, let's split up the token into its sections:

    String[] chunks = token.split("\\.");

    Our chunks array should now have two or three elements corresponding to the sections of the JWT.

    Base64.Decoder decoder = Base64.getDecoder();

    String header = new String(decoder.decode(chunks[0]));
    String payload = new String(decoder.decode(chunks[1]));
    Let's run this code with a JWT (we can decode online to compare results):

    eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkJhZWxkdW5nIFVzZXIiLCJpYXQiOjE1MTYyMzkwMjJ9.qH7Zj_m3kY69kxhaQXTa-ivIpytKXXjZc1ZSmapZnGE
    
    The output will give us the decoded header any payload:

    {"alg":"HS256","typ":"JWT"}<code class="language-java">{"sub":"1234567890","name":"Baeldung User","iat":1516239022}
    If only the header and payload sections are defined in a JWT, we are finished and have the information decoded successfully.


Q) Double-checked locking in singlton design pattern ?

Ans)

    Thread safe singlton:

        public class DraconianSingleton {
                private static DraconianSingleton instance;
                public static synchronized DraconianSingleton getInstance() {
                    if (instance == null) {
                        instance = new DraconianSingleton();
                    }
                    return instance;
                }

                // private constructor and other methods ...
            }

    Despite this class being thread-safe, we can see that there's a clear performance drawback: each time we want to get the instance of our singleton, we need to acquire a potentially unnecessary lock.

    To fix that, we could instead start by verifying if we need to create the object in the first place and only in that case we would acquire the lock.

    public class DclSingleton {
        
        private static volatile DclSingleton instance;
        public static DclSingleton getInstance() {
            if (instance == null) {
                synchronized (DclSingleton .class) {
                    if (instance == null) {
                        instance = new DclSingleton();
                    }
                }
            }
            return instance;
        }

        // private constructor and other methods...
    }
    
    One thing to keep in mind with this pattern is that the field needs to be volatile to prevent cache incoherence issues. 

Q) what will be the output of Math.min(Double.MIN_VALUE,0.0d) ??

ans)

    Double. MIN_VALUE is a constant holding the smallest POSITIVE nonzero value. So output Math.min(Double.MIN_VALUE,0.0d) will be 0.0

Q) How spring boot autconfiguration works internally ??

Ans) https://medium.com/empathyco/how-spring-boot-autoconfiguration-works-6e09f911c5ce

Q) What are generics and there performance implications ??

Ans)
    Generics is a compile time feature. It has next to no impact when running your application. No, it won't affect performance since it's not even there at runtime.

Q) How does saga pattern works ??

Ans)

    https://microservices.io/patterns/data/saga.html

Q) OOP's concept / SOLID principles (Associatation vs composition vs aggregation etc) 

Ans)

    https://www.nexsoftsys.com/articles/association-composition-aggregation-inheritance-java.html    


Q)  Explain about Solid principles and where you applied in projects??

Q)  Lot of questions on Multithreading framework?

Q)  Few programming questions based on Streams?

Q)  How to handle exceptions in microservices??

Q)  Create customized exception class?  In which scenario we use customized exception class

Q)  Swagger and it's important annotations??

Q) Executor framework?

Q)  difference between abstract class and interface?? 

Q)  When to choose arraylist over hashmap??


Q)  program to remove a character from string so that it becomes palindrome (read same forward and backword)??

Ans)

Q)  find longest palindrome in a string??

Q) How many microservices you have in your project ??

Ans)